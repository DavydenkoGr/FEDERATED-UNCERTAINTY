{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692de729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.utils import UEManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d370c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9788454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7fdbde91a390>]\n",
      "('sequence', 'GreedySemanticEnrichedPPLAveDissimilarity')\n",
      "('sequence', 'GreedySemanticEnrichedMaxprobAveDissimilarity')\n",
      "('sequence', 'GreedySemanticEnrichedMTEAveDissimilarity')\n",
      "('sequence', 'BestSemanticEnrichedPPLAveDissimilarity')\n",
      "('sequence', 'BestSemanticEnrichedMaxprobAveDissimilarity')\n",
      "('sequence', 'BestSemanticEnrichedMTEAveDissimilarity')\n",
      "('sequence', 'MbrSemanticEnrichedPPLAveDissimilarity')\n",
      "('sequence', 'MbrSemanticEnrichedMaxprobAveDissimilarity')\n",
      "('sequence', 'MbrSemanticEnrichedMTEAveDissimilarity')\n",
      "('sequence', 'SemanticEntropy')\n",
      "('sequence', 'SAR_t0.001')\n",
      "('sequence', 'MaximumSequenceProbability')\n",
      "('sequence', 'Perplexity')\n",
      "('sequence', 'MeanTokenEntropy')\n",
      "('sequence', 'BestSampledMaximumSequenceProbability')\n",
      "('sequence', 'BestSampledPerplexity')\n",
      "('sequence', 'BestSampledMeanTokenEntropy')\n",
      "('sequence', 'MbrSampledMaximumSequenceProbability')\n",
      "('sequence', 'MbrSampledPerplexity')\n",
      "('sequence', 'MbrSampledMeanTokenEntropy')\n",
      "('sequence', 'MonteCarloSequenceEntropy')\n",
      "('sequence', 'MonteCarloNormalizedSequenceEntropy')\n",
      "('sequence', 'CEDegMat')\n",
      "('sequence', 'GreedyAveDissimilarity')\n",
      "('sequence', 'BestAveDissimilarity')\n",
      "('sequence', 'MbrAveDissimilarity')\n",
      "('sequence', 'EigValLaplacian_NLI_score_entail')\n",
      "('sequence', 'DegMat_NLI_score_entail')\n"
     ]
    }
   ],
   "source": [
    "man_cal = UEManager.load('../resources/llm_resources/llama8b_xsum.man') # просто чтобы напечатать, какие меры неопределённости есть\n",
    "for el in man_cal.estimations.keys():\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564a6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdu.unc.entropic_ot import EntropicOTOrdering\n",
    "from mdu.unc.constants import OTTarget, SamplingMethod, ScalingType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46170e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# это можно не менять\n",
    "hyperparams_list = [\n",
    "    {\n",
    "        'target': OTTarget.EXP,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.FEATURE_WISE,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "    {\n",
    "        'target': OTTarget.EXP,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.GLOBAL,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "    {\n",
    "        'target': OTTarget.BETA,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.FEATURE_WISE,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "    {\n",
    "        'target': OTTarget.BETA,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.GLOBAL,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371a1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_ratio = 0.25 # это доля объектов, которая будет использована для калибровки (обучения ОТ). Её можно менять\n",
    "import pickle\n",
    "\n",
    "for model_name in [\"llama8b\", \"falcon7b\", \"mistral7b\"]:\n",
    "\n",
    "    file_names = [\n",
    "        f'{model_name}_xsum.man',\n",
    "        f'{model_name}_wmt14_fren.man',\n",
    "        f'{model_name}_wmt19_deen.man',\n",
    "        f'{model_name}_trivia.man',\n",
    "        f'{model_name}_mmlu.man',\n",
    "        f'{model_name}_gsm8k_cot.man',\n",
    "        f'{model_name}_coqa_no_context.man',\n",
    "    ]\n",
    "\n",
    "    for file_name in file_names:\n",
    "        man_current = UEManager.load(file_name)\n",
    "\n",
    "        measures_dict = {k: v for k, v in man_current.estimations.items()}\n",
    "\n",
    "        # Это я вручную создаю CoCoA меры (я проглядел, что они уже были в исходном менеджере, но под другими именами)\n",
    "        measures_dict[('sequence', 'CoCoA_greedy_MSP')] = man_current.estimations[('sequence', 'GreedyAveDissimilarity')] * man_current.estimations[('sequence', 'MaximumSequenceProbability')]\n",
    "        measures_dict[('sequence', 'CoCoA_greedy_PPL')] = man_current.estimations[('sequence', 'GreedyAveDissimilarity')] * man_current.estimations[('sequence', 'Perplexity')]\n",
    "        measures_dict[('sequence', 'CoCoA_greedy_MTE')] = man_current.estimations[('sequence', 'GreedyAveDissimilarity')] * man_current.estimations[('sequence', 'MeanTokenEntropy')]\n",
    "\n",
    "        measures_dict[('sequence', 'CoCoA_best_MSP')] = man_current.estimations[('sequence', 'BestAveDissimilarity')] * man_current.estimations[('sequence', 'BestSampledMaximumSequenceProbability')]\n",
    "        measures_dict[('sequence', 'CoCoA_best_PPL')] = man_current.estimations[('sequence', 'BestAveDissimilarity')] * man_current.estimations[('sequence', 'BestSampledPerplexity')]\n",
    "        measures_dict[('sequence', 'CoCoA_best_MTE')] = man_current.estimations[('sequence', 'BestAveDissimilarity')] * man_current.estimations[('sequence', 'BestSampledMeanTokenEntropy')]\n",
    "\n",
    "        n_samples = len(man_current.estimations[('sequence', 'Perplexity')])\n",
    "        np.random.seed(42)\n",
    "        # Это я фиксирую калибровочную маску\n",
    "        calib_mask = np.random.choice(n_samples, size=int(n_samples * calib_ratio), replace=False)\n",
    "\n",
    "        final_measures = {}\n",
    "\n",
    "        final_measures['calibration_mask'] = calib_mask\n",
    "\n",
    "        for hyperparams in hyperparams_list:\n",
    "            target = hyperparams['target']\n",
    "            sampling_method = hyperparams['sampling_method']\n",
    "            scaling_type = hyperparams['scaling_type']\n",
    "            grid_size = hyperparams['grid_size']\n",
    "            n_targets_multiplier = hyperparams['n_targets_multiplier']\n",
    "            eps = hyperparams['eps']\n",
    "            max_iters = hyperparams['max_iters']\n",
    "            tol = hyperparams['tol']\n",
    "\n",
    "\n",
    "            model = EntropicOTOrdering(\n",
    "                target=target,\n",
    "                sampling_method=sampling_method,\n",
    "                scaling_type=scaling_type,\n",
    "                grid_size=grid_size,\n",
    "                target_params={},\n",
    "                eps=eps,\n",
    "                n_targets_multiplier=n_targets_multiplier,\n",
    "                max_iters=max_iters,\n",
    "                random_state=42,\n",
    "                tol=tol,\n",
    "            )\n",
    "\n",
    "            combinations = [\n",
    "                (\n",
    "                    f\"CoCoA_like_1_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера\n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'GreedyAveDissimilarity'),\n",
    "                        ('sequence', 'BestAveDissimilarity'),\n",
    "                        ('sequence', 'BestSampledMaximumSequenceProbability'),\n",
    "                        ('sequence', 'BestSampledPerplexity'),\n",
    "                        ('sequence', 'BestSampledMeanTokenEntropy'),\n",
    "                        ('sequence', 'MaximumSequenceProbability'),\n",
    "                        ('sequence', 'Perplexity'),\n",
    "                        ('sequence', 'MeanTokenEntropy'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"CoCoA_like_2_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера\n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'BestAveDissimilarity'),\n",
    "                        ('sequence', 'BestSampledMaximumSequenceProbability'),\n",
    "                        ('sequence', 'BestSampledPerplexity'),\n",
    "                        ('sequence', 'BestSampledMeanTokenEntropy'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"CoCoA_like_3_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'GreedyAveDissimilarity'),\n",
    "                        ('sequence', 'MaximumSequenceProbability'),\n",
    "                        ('sequence', 'Perplexity'),\n",
    "                        ('sequence', 'MeanTokenEntropy'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"Master_1_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                        ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                        ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"Master_1_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                        ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                        ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"Master_2_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_best_MSP'),\n",
    "                        ('sequence', 'CoCoA_best_PPL'),\n",
    "                        ('sequence', 'CoCoA_best_MTE'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"Master_3_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_best_MSP'),\n",
    "                        ('sequence', 'CoCoA_best_PPL'),\n",
    "                        ('sequence', 'CoCoA_best_MTE'),\n",
    "                        ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                        ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                        ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"IMBA_1_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_best_MSP'),\n",
    "                        ('sequence', 'CoCoA_best_PPL'),\n",
    "                        ('sequence', 'DegMat_NLI_score_entail'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"IMBA_2_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                        ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                        ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                        ('sequence', 'DegMat_NLI_score_entail'),  # это имя, под которым будет сохраняться композитная мера\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"IMBA_3_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_best_MSP'),\n",
    "                        ('sequence', 'CoCoA_best_PPL'),\n",
    "                        ('sequence', 'CoCoA_best_MTE'),\n",
    "                        ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                        ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                        ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                        ('sequence', 'DegMat_NLI_score_entail'),\n",
    "                        ('sequence', 'EigValLaplacian_NLI_score_entail'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"IMBA_4_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                        ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                        ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                        ('sequence', 'DegMat_NLI_score_entail'),\n",
    "                        ('sequence', 'MaximumSequenceProbability'),\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    f\"IMBA_5_{target.value}_{scaling_type.value}\",  # это имя, под которым будет сохраняться композитная мера \n",
    "                    (# а дальше в этом кортеже просто напихиваешь те unc меры, которые хочешь объединить\n",
    "                        ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                        ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                        ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                        ('sequence', 'MaximumSequenceProbability'),\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            for comb_list in combinations:\n",
    "                name, comb = comb_list[0], comb_list[1]\n",
    "                components = [measures_dict[el] for el in comb]\n",
    "                \n",
    "                calib_components = [np.array(comp)[calib_mask] for comp in components]\n",
    "\n",
    "                stacked_calib_components = np.column_stack(calib_components)\n",
    "                stacked_test_components = np.column_stack(components)\n",
    "\n",
    "                model.fit(stacked_calib_components)\n",
    "                scores = model.predict(stacked_test_components)\n",
    "\n",
    "                final_measures[name] = scores\n",
    "\n",
    "        with open(f'../resources/llm_resources/final_measures_{file_name.split(\".\")[0]}.pkl', 'wb') as f:\n",
    "            pickle.dump(final_measures, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d22ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b492b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from lm_polygraph.utils import UEManager\n",
    "\n",
    "models = ['llama8b', 'falcon7b','mistral7b']\n",
    "datasets = ['trivia', 'mmlu','wmt14_fren','wmt19_deen','xsum','gsm8k_cot','coqa_no_context']\n",
    "results = []\n",
    "quality_metrics={\n",
    "    'trivia':'AlignScoreOutputTarget',\n",
    "    'mmlu':'Accuracy',\n",
    "    'wmt14_fren':'Comet',\n",
    "    'wmt19_deen':'Comet',\n",
    "    'xsum':'AlignScoreInputOutput',\n",
    "    'gsm8k_cot':'Accuracy',\n",
    "    'coqa_no_context':'AlignScoreOutputTarget'\n",
    "}\n",
    "for model in models: \n",
    "    for dataset in datasets:\n",
    "        man = UEManager.load(f'../resources/llm_resources/{model}_{dataset}.man')\n",
    "\n",
    "        with open(f'../resources/llm_resources/final_measures_{model}_{dataset}.pkl', 'rb') as f:\n",
    "            final_measures_1 = pickle.load(f)\n",
    "        for method in list(final_measures_1.keys()):\n",
    "            if method!='calibration_mask':\n",
    "                man.estimations[('sequence', method)] = final_measures_1[method]\n",
    "        \n",
    "        calibration_mask = final_measures_1['calibration_mask']\n",
    "        for key, values in man.estimations.items():\n",
    "            man.estimations[key] = [v for i, v in enumerate(values) if i not in calibration_mask]\n",
    "        \n",
    "        for key, values in man.gen_metrics.items():\n",
    "            man.gen_metrics[key] = [v for i, v in enumerate(values) if i not in calibration_mask]\n",
    "\n",
    "        from lm_polygraph.ue_metrics import PredictionRejectionArea\n",
    "\n",
    "        ue_metrics =[PredictionRejectionArea(max_rejection=0.5)]\n",
    "\n",
    "\n",
    "        man.ue_metrics=ue_metrics\n",
    "\n",
    "        man.eval_ue()\n",
    "        quality_metric =quality_metrics[dataset]\n",
    "        methods = [x[1] for x in list(man.estimations.keys())]\n",
    "\n",
    "        methods = [\n",
    "            method\n",
    "            for method in methods\n",
    "            if \"mbr\" not in method.lower() and \"sample\" not in method.lower() and \"best\" not in method.lower()\n",
    "        ]\n",
    "\n",
    "        \n",
    "        for method in methods:\n",
    "            prr_score = man.metrics[('sequence', method, quality_metric, 'prr_0.5_normalized')]\n",
    "            results.append({ 'model': model, 'Method':method, 'prr': prr_score, 'dataset':dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d27b8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>Method</th>\n",
       "      <th>coqa_no_context</th>\n",
       "      <th>gsm8k_cot</th>\n",
       "      <th>mmlu</th>\n",
       "      <th>trivia</th>\n",
       "      <th>wmt14_fren</th>\n",
       "      <th>wmt19_deen</th>\n",
       "      <th>xsum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMBA_5_Exp_FeatureWise</td>\n",
       "      <td>0.353907</td>\n",
       "      <td>0.419594</td>\n",
       "      <td>0.484351</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.475661</td>\n",
       "      <td>0.612408</td>\n",
       "      <td>0.405422</td>\n",
       "      <td>0.478349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMBA_5_Beta_FeatureWise</td>\n",
       "      <td>0.355931</td>\n",
       "      <td>0.409402</td>\n",
       "      <td>0.481497</td>\n",
       "      <td>0.599280</td>\n",
       "      <td>0.475688</td>\n",
       "      <td>0.613109</td>\n",
       "      <td>0.409612</td>\n",
       "      <td>0.477789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Master_1_Exp_FeatureWise</td>\n",
       "      <td>0.356833</td>\n",
       "      <td>0.438419</td>\n",
       "      <td>0.461269</td>\n",
       "      <td>0.605990</td>\n",
       "      <td>0.470549</td>\n",
       "      <td>0.565129</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>0.472369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Master_1_Beta_FeatureWise</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>0.430522</td>\n",
       "      <td>0.460514</td>\n",
       "      <td>0.606566</td>\n",
       "      <td>0.470590</td>\n",
       "      <td>0.564865</td>\n",
       "      <td>0.410731</td>\n",
       "      <td>0.471623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Master_1_Exp_Global</td>\n",
       "      <td>0.368312</td>\n",
       "      <td>0.366505</td>\n",
       "      <td>0.463711</td>\n",
       "      <td>0.608272</td>\n",
       "      <td>0.459690</td>\n",
       "      <td>0.595384</td>\n",
       "      <td>0.400275</td>\n",
       "      <td>0.466021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Master_1_Beta_Global</td>\n",
       "      <td>0.368916</td>\n",
       "      <td>0.365808</td>\n",
       "      <td>0.463258</td>\n",
       "      <td>0.608094</td>\n",
       "      <td>0.458775</td>\n",
       "      <td>0.595235</td>\n",
       "      <td>0.400645</td>\n",
       "      <td>0.465819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GreedySemanticEnrichedMaxprobAveDissimilarity</td>\n",
       "      <td>0.366889</td>\n",
       "      <td>0.362325</td>\n",
       "      <td>0.491508</td>\n",
       "      <td>0.602602</td>\n",
       "      <td>0.442589</td>\n",
       "      <td>0.584028</td>\n",
       "      <td>0.394897</td>\n",
       "      <td>0.463548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IMBA_2_Beta_Global</td>\n",
       "      <td>0.376108</td>\n",
       "      <td>0.368005</td>\n",
       "      <td>0.461837</td>\n",
       "      <td>0.614290</td>\n",
       "      <td>0.440997</td>\n",
       "      <td>0.577845</td>\n",
       "      <td>0.400559</td>\n",
       "      <td>0.462806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IMBA_2_Exp_Global</td>\n",
       "      <td>0.377028</td>\n",
       "      <td>0.371017</td>\n",
       "      <td>0.461118</td>\n",
       "      <td>0.616403</td>\n",
       "      <td>0.438409</td>\n",
       "      <td>0.573779</td>\n",
       "      <td>0.401458</td>\n",
       "      <td>0.462745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IMBA_4_Exp_FeatureWise</td>\n",
       "      <td>0.379458</td>\n",
       "      <td>0.430302</td>\n",
       "      <td>0.478185</td>\n",
       "      <td>0.619169</td>\n",
       "      <td>0.344328</td>\n",
       "      <td>0.563956</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>0.460839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CoCoA_like_3_Beta_FeatureWise</td>\n",
       "      <td>0.336145</td>\n",
       "      <td>0.409820</td>\n",
       "      <td>0.475266</td>\n",
       "      <td>0.594859</td>\n",
       "      <td>0.439068</td>\n",
       "      <td>0.576805</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.460035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IMBA_4_Beta_FeatureWise</td>\n",
       "      <td>0.379941</td>\n",
       "      <td>0.419805</td>\n",
       "      <td>0.476333</td>\n",
       "      <td>0.619484</td>\n",
       "      <td>0.335432</td>\n",
       "      <td>0.559856</td>\n",
       "      <td>0.414785</td>\n",
       "      <td>0.457948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CoCoA_like_3_Exp_FeatureWise</td>\n",
       "      <td>0.339954</td>\n",
       "      <td>0.419487</td>\n",
       "      <td>0.478852</td>\n",
       "      <td>0.591325</td>\n",
       "      <td>0.441541</td>\n",
       "      <td>0.576132</td>\n",
       "      <td>0.335898</td>\n",
       "      <td>0.454741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GreedySemanticEnrichedPPLAveDissimilarity</td>\n",
       "      <td>0.351629</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.457640</td>\n",
       "      <td>0.597873</td>\n",
       "      <td>0.439909</td>\n",
       "      <td>0.508992</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.453481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IMBA_2_Exp_FeatureWise</td>\n",
       "      <td>0.382852</td>\n",
       "      <td>0.435330</td>\n",
       "      <td>0.455928</td>\n",
       "      <td>0.625140</td>\n",
       "      <td>0.319405</td>\n",
       "      <td>0.518174</td>\n",
       "      <td>0.406742</td>\n",
       "      <td>0.449082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IMBA_2_Beta_FeatureWise</td>\n",
       "      <td>0.380607</td>\n",
       "      <td>0.428647</td>\n",
       "      <td>0.457606</td>\n",
       "      <td>0.623677</td>\n",
       "      <td>0.318293</td>\n",
       "      <td>0.523291</td>\n",
       "      <td>0.410099</td>\n",
       "      <td>0.448889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GreedySemanticEnrichedMTEAveDissimilarity</td>\n",
       "      <td>0.350511</td>\n",
       "      <td>0.433381</td>\n",
       "      <td>0.408473</td>\n",
       "      <td>0.603853</td>\n",
       "      <td>0.435925</td>\n",
       "      <td>0.505174</td>\n",
       "      <td>0.392047</td>\n",
       "      <td>0.447052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Master_3_Beta_FeatureWise</td>\n",
       "      <td>0.356386</td>\n",
       "      <td>0.411995</td>\n",
       "      <td>0.455742</td>\n",
       "      <td>0.601985</td>\n",
       "      <td>0.450759</td>\n",
       "      <td>0.582538</td>\n",
       "      <td>0.255601</td>\n",
       "      <td>0.445001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Master_3_Exp_FeatureWise</td>\n",
       "      <td>0.356415</td>\n",
       "      <td>0.408491</td>\n",
       "      <td>0.456057</td>\n",
       "      <td>0.600984</td>\n",
       "      <td>0.450969</td>\n",
       "      <td>0.580766</td>\n",
       "      <td>0.247672</td>\n",
       "      <td>0.443051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CoCoA_like_1_Beta_FeatureWise</td>\n",
       "      <td>0.340199</td>\n",
       "      <td>0.395301</td>\n",
       "      <td>0.472162</td>\n",
       "      <td>0.592579</td>\n",
       "      <td>0.438466</td>\n",
       "      <td>0.611143</td>\n",
       "      <td>0.210464</td>\n",
       "      <td>0.437188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CoCoA_like_1_Exp_FeatureWise</td>\n",
       "      <td>0.340182</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>0.476597</td>\n",
       "      <td>0.591501</td>\n",
       "      <td>0.433983</td>\n",
       "      <td>0.602976</td>\n",
       "      <td>0.202109</td>\n",
       "      <td>0.434534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IMBA_4_Exp_Global</td>\n",
       "      <td>0.350788</td>\n",
       "      <td>0.338917</td>\n",
       "      <td>0.489219</td>\n",
       "      <td>0.600092</td>\n",
       "      <td>0.354359</td>\n",
       "      <td>0.524928</td>\n",
       "      <td>0.374095</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Master_3_Exp_Global</td>\n",
       "      <td>0.362935</td>\n",
       "      <td>0.354143</td>\n",
       "      <td>0.459075</td>\n",
       "      <td>0.604965</td>\n",
       "      <td>0.431890</td>\n",
       "      <td>0.594252</td>\n",
       "      <td>0.217942</td>\n",
       "      <td>0.432172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>IMBA_4_Beta_Global</td>\n",
       "      <td>0.348072</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>0.487914</td>\n",
       "      <td>0.597838</td>\n",
       "      <td>0.349689</td>\n",
       "      <td>0.515189</td>\n",
       "      <td>0.373197</td>\n",
       "      <td>0.429187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Master_3_Beta_Global</td>\n",
       "      <td>0.364434</td>\n",
       "      <td>0.351208</td>\n",
       "      <td>0.459089</td>\n",
       "      <td>0.605563</td>\n",
       "      <td>0.423418</td>\n",
       "      <td>0.591117</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.428921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IMBA_5_Exp_Global</td>\n",
       "      <td>0.343138</td>\n",
       "      <td>0.333372</td>\n",
       "      <td>0.492882</td>\n",
       "      <td>0.589422</td>\n",
       "      <td>0.350708</td>\n",
       "      <td>0.515644</td>\n",
       "      <td>0.373645</td>\n",
       "      <td>0.428402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IMBA_5_Beta_Global</td>\n",
       "      <td>0.341805</td>\n",
       "      <td>0.331868</td>\n",
       "      <td>0.491450</td>\n",
       "      <td>0.589069</td>\n",
       "      <td>0.347590</td>\n",
       "      <td>0.510569</td>\n",
       "      <td>0.373062</td>\n",
       "      <td>0.426488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IMBA_3_Beta_FeatureWise</td>\n",
       "      <td>0.376074</td>\n",
       "      <td>0.419893</td>\n",
       "      <td>0.453696</td>\n",
       "      <td>0.623715</td>\n",
       "      <td>0.316124</td>\n",
       "      <td>0.517650</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.424124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IMBA_3_Exp_FeatureWise</td>\n",
       "      <td>0.377913</td>\n",
       "      <td>0.426849</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>0.623697</td>\n",
       "      <td>0.315689</td>\n",
       "      <td>0.511032</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.423218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>IMBA_3_Beta_Global</td>\n",
       "      <td>0.375223</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.453754</td>\n",
       "      <td>0.626566</td>\n",
       "      <td>0.381189</td>\n",
       "      <td>0.558364</td>\n",
       "      <td>0.209408</td>\n",
       "      <td>0.422185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>IMBA_3_Exp_Global</td>\n",
       "      <td>0.373297</td>\n",
       "      <td>0.363378</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.624717</td>\n",
       "      <td>0.360944</td>\n",
       "      <td>0.541079</td>\n",
       "      <td>0.199686</td>\n",
       "      <td>0.416643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CoCoA_like_3_Exp_Global</td>\n",
       "      <td>0.314715</td>\n",
       "      <td>0.317123</td>\n",
       "      <td>0.488686</td>\n",
       "      <td>0.572646</td>\n",
       "      <td>0.344201</td>\n",
       "      <td>0.516505</td>\n",
       "      <td>0.352260</td>\n",
       "      <td>0.415162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CoCoA_like_3_Beta_Global</td>\n",
       "      <td>0.315034</td>\n",
       "      <td>0.314538</td>\n",
       "      <td>0.486222</td>\n",
       "      <td>0.570157</td>\n",
       "      <td>0.338264</td>\n",
       "      <td>0.505950</td>\n",
       "      <td>0.352009</td>\n",
       "      <td>0.411739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CoCoA_like_1_Exp_Global</td>\n",
       "      <td>0.321302</td>\n",
       "      <td>0.326569</td>\n",
       "      <td>0.480832</td>\n",
       "      <td>0.583840</td>\n",
       "      <td>0.343926</td>\n",
       "      <td>0.540315</td>\n",
       "      <td>0.226889</td>\n",
       "      <td>0.403382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MaximumSequenceProbability</td>\n",
       "      <td>0.290338</td>\n",
       "      <td>0.309910</td>\n",
       "      <td>0.516374</td>\n",
       "      <td>0.538346</td>\n",
       "      <td>0.320281</td>\n",
       "      <td>0.468913</td>\n",
       "      <td>0.341465</td>\n",
       "      <td>0.397947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CoCoA_like_1_Beta_Global</td>\n",
       "      <td>0.317172</td>\n",
       "      <td>0.307602</td>\n",
       "      <td>0.483145</td>\n",
       "      <td>0.574003</td>\n",
       "      <td>0.321527</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>0.210551</td>\n",
       "      <td>0.386879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Master_2_Beta_FeatureWise</td>\n",
       "      <td>0.346188</td>\n",
       "      <td>0.309646</td>\n",
       "      <td>0.443848</td>\n",
       "      <td>0.590745</td>\n",
       "      <td>0.415769</td>\n",
       "      <td>0.567580</td>\n",
       "      <td>0.017416</td>\n",
       "      <td>0.384456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Master_2_Exp_FeatureWise</td>\n",
       "      <td>0.346868</td>\n",
       "      <td>0.308184</td>\n",
       "      <td>0.444368</td>\n",
       "      <td>0.590386</td>\n",
       "      <td>0.416865</td>\n",
       "      <td>0.565201</td>\n",
       "      <td>0.017066</td>\n",
       "      <td>0.384134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IMBA_1_Exp_Global</td>\n",
       "      <td>0.370339</td>\n",
       "      <td>0.274232</td>\n",
       "      <td>0.467714</td>\n",
       "      <td>0.607525</td>\n",
       "      <td>0.374167</td>\n",
       "      <td>0.550068</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>0.381062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>IMBA_1_Beta_Global</td>\n",
       "      <td>0.368736</td>\n",
       "      <td>0.271991</td>\n",
       "      <td>0.468239</td>\n",
       "      <td>0.605630</td>\n",
       "      <td>0.374637</td>\n",
       "      <td>0.551691</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>0.380701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Perplexity</td>\n",
       "      <td>0.260974</td>\n",
       "      <td>0.283814</td>\n",
       "      <td>0.468947</td>\n",
       "      <td>0.519790</td>\n",
       "      <td>0.345569</td>\n",
       "      <td>0.402386</td>\n",
       "      <td>0.382807</td>\n",
       "      <td>0.380612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Master_2_Exp_Global</td>\n",
       "      <td>0.353573</td>\n",
       "      <td>0.278936</td>\n",
       "      <td>0.449461</td>\n",
       "      <td>0.597258</td>\n",
       "      <td>0.388970</td>\n",
       "      <td>0.558942</td>\n",
       "      <td>0.021147</td>\n",
       "      <td>0.378327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Master_2_Beta_Global</td>\n",
       "      <td>0.353239</td>\n",
       "      <td>0.275592</td>\n",
       "      <td>0.449185</td>\n",
       "      <td>0.597174</td>\n",
       "      <td>0.386715</td>\n",
       "      <td>0.558175</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.377485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>CoCoA_like_2_Beta_FeatureWise</td>\n",
       "      <td>0.330824</td>\n",
       "      <td>0.297892</td>\n",
       "      <td>0.459112</td>\n",
       "      <td>0.581284</td>\n",
       "      <td>0.390238</td>\n",
       "      <td>0.570247</td>\n",
       "      <td>0.011944</td>\n",
       "      <td>0.377363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>CoCoA_like_2_Exp_FeatureWise</td>\n",
       "      <td>0.333256</td>\n",
       "      <td>0.279902</td>\n",
       "      <td>0.462255</td>\n",
       "      <td>0.577841</td>\n",
       "      <td>0.391132</td>\n",
       "      <td>0.562686</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.374245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SAR_t0.001</td>\n",
       "      <td>0.323909</td>\n",
       "      <td>0.389040</td>\n",
       "      <td>0.360104</td>\n",
       "      <td>0.592717</td>\n",
       "      <td>0.412148</td>\n",
       "      <td>0.472180</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>0.372411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>GreedyAveDissimilarity</td>\n",
       "      <td>0.391002</td>\n",
       "      <td>0.366882</td>\n",
       "      <td>0.395135</td>\n",
       "      <td>0.615159</td>\n",
       "      <td>0.376409</td>\n",
       "      <td>0.450369</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>0.371770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IMBA_1_Beta_FeatureWise</td>\n",
       "      <td>0.374696</td>\n",
       "      <td>0.324490</td>\n",
       "      <td>0.460181</td>\n",
       "      <td>0.614801</td>\n",
       "      <td>0.287036</td>\n",
       "      <td>0.461238</td>\n",
       "      <td>0.040546</td>\n",
       "      <td>0.366141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>IMBA_1_Exp_FeatureWise</td>\n",
       "      <td>0.376030</td>\n",
       "      <td>0.322517</td>\n",
       "      <td>0.457755</td>\n",
       "      <td>0.616460</td>\n",
       "      <td>0.288181</td>\n",
       "      <td>0.458103</td>\n",
       "      <td>0.038234</td>\n",
       "      <td>0.365326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MeanTokenEntropy</td>\n",
       "      <td>0.247590</td>\n",
       "      <td>0.308429</td>\n",
       "      <td>0.362241</td>\n",
       "      <td>0.505860</td>\n",
       "      <td>0.357613</td>\n",
       "      <td>0.390544</td>\n",
       "      <td>0.367250</td>\n",
       "      <td>0.362790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>CEDegMat</td>\n",
       "      <td>0.356052</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>0.335752</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>0.311105</td>\n",
       "      <td>0.395779</td>\n",
       "      <td>0.067962</td>\n",
       "      <td>0.338441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>CoCoA_like_2_Exp_Global</td>\n",
       "      <td>0.302724</td>\n",
       "      <td>0.235115</td>\n",
       "      <td>0.477230</td>\n",
       "      <td>0.562359</td>\n",
       "      <td>0.289802</td>\n",
       "      <td>0.444612</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.332078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>CoCoA_like_2_Beta_Global</td>\n",
       "      <td>0.305148</td>\n",
       "      <td>0.227752</td>\n",
       "      <td>0.474705</td>\n",
       "      <td>0.562225</td>\n",
       "      <td>0.283805</td>\n",
       "      <td>0.435249</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.328941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>DegMat_NLI_score_entail</td>\n",
       "      <td>0.369725</td>\n",
       "      <td>0.294448</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>0.615934</td>\n",
       "      <td>0.220236</td>\n",
       "      <td>0.356593</td>\n",
       "      <td>0.075562</td>\n",
       "      <td>0.325501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SemanticEntropy</td>\n",
       "      <td>0.289091</td>\n",
       "      <td>0.383727</td>\n",
       "      <td>0.234864</td>\n",
       "      <td>0.540778</td>\n",
       "      <td>0.277182</td>\n",
       "      <td>0.410090</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.309193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>MonteCarloNormalizedSequenceEntropy</td>\n",
       "      <td>0.235884</td>\n",
       "      <td>0.349142</td>\n",
       "      <td>0.171010</td>\n",
       "      <td>0.499136</td>\n",
       "      <td>0.360110</td>\n",
       "      <td>0.406894</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.291157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>EigValLaplacian_NLI_score_entail</td>\n",
       "      <td>0.346395</td>\n",
       "      <td>0.263502</td>\n",
       "      <td>0.295546</td>\n",
       "      <td>0.599807</td>\n",
       "      <td>0.152410</td>\n",
       "      <td>0.283339</td>\n",
       "      <td>0.075191</td>\n",
       "      <td>0.288027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>MonteCarloSequenceEntropy</td>\n",
       "      <td>0.249878</td>\n",
       "      <td>0.373571</td>\n",
       "      <td>0.173789</td>\n",
       "      <td>0.474019</td>\n",
       "      <td>0.274102</td>\n",
       "      <td>0.383992</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.279593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                                         Method  coqa_no_context  \\\n",
       "0                               IMBA_5_Exp_FeatureWise         0.353907   \n",
       "1                              IMBA_5_Beta_FeatureWise         0.355931   \n",
       "2                             Master_1_Exp_FeatureWise         0.356833   \n",
       "3                            Master_1_Beta_FeatureWise         0.357576   \n",
       "4                                  Master_1_Exp_Global         0.368312   \n",
       "5                                 Master_1_Beta_Global         0.368916   \n",
       "6        GreedySemanticEnrichedMaxprobAveDissimilarity         0.366889   \n",
       "7                                   IMBA_2_Beta_Global         0.376108   \n",
       "8                                    IMBA_2_Exp_Global         0.377028   \n",
       "9                               IMBA_4_Exp_FeatureWise         0.379458   \n",
       "10                       CoCoA_like_3_Beta_FeatureWise         0.336145   \n",
       "11                             IMBA_4_Beta_FeatureWise         0.379941   \n",
       "12                        CoCoA_like_3_Exp_FeatureWise         0.339954   \n",
       "13           GreedySemanticEnrichedPPLAveDissimilarity         0.351629   \n",
       "14                              IMBA_2_Exp_FeatureWise         0.382852   \n",
       "15                             IMBA_2_Beta_FeatureWise         0.380607   \n",
       "16           GreedySemanticEnrichedMTEAveDissimilarity         0.350511   \n",
       "17                           Master_3_Beta_FeatureWise         0.356386   \n",
       "18                            Master_3_Exp_FeatureWise         0.356415   \n",
       "19                       CoCoA_like_1_Beta_FeatureWise         0.340199   \n",
       "20                        CoCoA_like_1_Exp_FeatureWise         0.340182   \n",
       "21                                   IMBA_4_Exp_Global         0.350788   \n",
       "22                                 Master_3_Exp_Global         0.362935   \n",
       "23                                  IMBA_4_Beta_Global         0.348072   \n",
       "24                                Master_3_Beta_Global         0.364434   \n",
       "25                                   IMBA_5_Exp_Global         0.343138   \n",
       "26                                  IMBA_5_Beta_Global         0.341805   \n",
       "27                             IMBA_3_Beta_FeatureWise         0.376074   \n",
       "28                              IMBA_3_Exp_FeatureWise         0.377913   \n",
       "29                                  IMBA_3_Beta_Global         0.375223   \n",
       "30                                   IMBA_3_Exp_Global         0.373297   \n",
       "31                             CoCoA_like_3_Exp_Global         0.314715   \n",
       "32                            CoCoA_like_3_Beta_Global         0.315034   \n",
       "33                             CoCoA_like_1_Exp_Global         0.321302   \n",
       "34                          MaximumSequenceProbability         0.290338   \n",
       "35                            CoCoA_like_1_Beta_Global         0.317172   \n",
       "36                           Master_2_Beta_FeatureWise         0.346188   \n",
       "37                            Master_2_Exp_FeatureWise         0.346868   \n",
       "38                                   IMBA_1_Exp_Global         0.370339   \n",
       "39                                  IMBA_1_Beta_Global         0.368736   \n",
       "40                                          Perplexity         0.260974   \n",
       "41                                 Master_2_Exp_Global         0.353573   \n",
       "42                                Master_2_Beta_Global         0.353239   \n",
       "43                       CoCoA_like_2_Beta_FeatureWise         0.330824   \n",
       "44                        CoCoA_like_2_Exp_FeatureWise         0.333256   \n",
       "45                                          SAR_t0.001         0.323909   \n",
       "46                              GreedyAveDissimilarity         0.391002   \n",
       "47                             IMBA_1_Beta_FeatureWise         0.374696   \n",
       "48                              IMBA_1_Exp_FeatureWise         0.376030   \n",
       "49                                    MeanTokenEntropy         0.247590   \n",
       "50                                            CEDegMat         0.356052   \n",
       "51                             CoCoA_like_2_Exp_Global         0.302724   \n",
       "52                            CoCoA_like_2_Beta_Global         0.305148   \n",
       "53                             DegMat_NLI_score_entail         0.369725   \n",
       "54                                     SemanticEntropy         0.289091   \n",
       "55                 MonteCarloNormalizedSequenceEntropy         0.235884   \n",
       "56                    EigValLaplacian_NLI_score_entail         0.346395   \n",
       "57                           MonteCarloSequenceEntropy         0.249878   \n",
       "\n",
       "dataset  gsm8k_cot      mmlu    trivia  wmt14_fren  wmt19_deen      xsum  \\\n",
       "0         0.419594  0.484351  0.597100    0.475661    0.612408  0.405422   \n",
       "1         0.409402  0.481497  0.599280    0.475688    0.613109  0.409612   \n",
       "2         0.438419  0.461269  0.605990    0.470549    0.565129  0.408392   \n",
       "3         0.430522  0.460514  0.606566    0.470590    0.564865  0.410731   \n",
       "4         0.366505  0.463711  0.608272    0.459690    0.595384  0.400275   \n",
       "5         0.365808  0.463258  0.608094    0.458775    0.595235  0.400645   \n",
       "6         0.362325  0.491508  0.602602    0.442589    0.584028  0.394897   \n",
       "7         0.368005  0.461837  0.614290    0.440997    0.577845  0.400559   \n",
       "8         0.371017  0.461118  0.616403    0.438409    0.573779  0.401458   \n",
       "9         0.430302  0.478185  0.619169    0.344328    0.563956  0.410475   \n",
       "10        0.409820  0.475266  0.594859    0.439068    0.576805  0.388284   \n",
       "11        0.419805  0.476333  0.619484    0.335432    0.559856  0.414785   \n",
       "12        0.419487  0.478852  0.591325    0.441541    0.576132  0.335898   \n",
       "13        0.417121  0.457640  0.597873    0.439909    0.508992  0.401200   \n",
       "14        0.435330  0.455928  0.625140    0.319405    0.518174  0.406742   \n",
       "15        0.428647  0.457606  0.623677    0.318293    0.523291  0.410099   \n",
       "16        0.433381  0.408473  0.603853    0.435925    0.505174  0.392047   \n",
       "17        0.411995  0.455742  0.601985    0.450759    0.582538  0.255601   \n",
       "18        0.408491  0.456057  0.600984    0.450969    0.580766  0.247672   \n",
       "19        0.395301  0.472162  0.592579    0.438466    0.611143  0.210464   \n",
       "20        0.394394  0.476597  0.591501    0.433983    0.602976  0.202109   \n",
       "21        0.338917  0.489219  0.600092    0.354359    0.524928  0.374095   \n",
       "22        0.354143  0.459075  0.604965    0.431890    0.594252  0.217942   \n",
       "23        0.332410  0.487914  0.597838    0.349689    0.515189  0.373197   \n",
       "24        0.351208  0.459089  0.605563    0.423418    0.591117  0.207621   \n",
       "25        0.333372  0.492882  0.589422    0.350708    0.515644  0.373645   \n",
       "26        0.331868  0.491450  0.589069    0.347590    0.510569  0.373062   \n",
       "27        0.419893  0.453696  0.623715    0.316124    0.517650  0.261719   \n",
       "28        0.426849  0.452142  0.623697    0.315689    0.511032  0.255200   \n",
       "29        0.350789  0.453754  0.626566    0.381189    0.558364  0.209408   \n",
       "30        0.363378  0.453400  0.624717    0.360944    0.541079  0.199686   \n",
       "31        0.317123  0.488686  0.572646    0.344201    0.516505  0.352260   \n",
       "32        0.314538  0.486222  0.570157    0.338264    0.505950  0.352009   \n",
       "33        0.326569  0.480832  0.583840    0.343926    0.540315  0.226889   \n",
       "34        0.309910  0.516374  0.538346    0.320281    0.468913  0.341465   \n",
       "35        0.307602  0.483145  0.574003    0.321527    0.494155  0.210551   \n",
       "36        0.309646  0.443848  0.590745    0.415769    0.567580  0.017416   \n",
       "37        0.308184  0.444368  0.590386    0.416865    0.565201  0.017066   \n",
       "38        0.274232  0.467714  0.607525    0.374167    0.550068  0.023388   \n",
       "39        0.271991  0.468239  0.605630    0.374637    0.551691  0.023986   \n",
       "40        0.283814  0.468947  0.519790    0.345569    0.402386  0.382807   \n",
       "41        0.278936  0.449461  0.597258    0.388970    0.558942  0.021147   \n",
       "42        0.275592  0.449185  0.597174    0.386715    0.558175  0.022315   \n",
       "43        0.297892  0.459112  0.581284    0.390238    0.570247  0.011944   \n",
       "44        0.279902  0.462255  0.577841    0.391132    0.562686  0.012644   \n",
       "45        0.389040  0.360104  0.592717    0.412148    0.472180  0.056778   \n",
       "46        0.366882  0.395135  0.615159    0.376409    0.450369  0.007431   \n",
       "47        0.324490  0.460181  0.614801    0.287036    0.461238  0.040546   \n",
       "48        0.322517  0.457755  0.616460    0.288181    0.458103  0.038234   \n",
       "49        0.308429  0.362241  0.505860    0.357613    0.390544  0.367250   \n",
       "50        0.298368  0.335752  0.604070    0.311105    0.395779  0.067962   \n",
       "51        0.235115  0.477230  0.562359    0.289802    0.444612  0.012702   \n",
       "52        0.227752  0.474705  0.562225    0.283805    0.435249  0.013703   \n",
       "53        0.294448  0.346010  0.615934    0.220236    0.356593  0.075562   \n",
       "54        0.383727  0.234864  0.540778    0.277182    0.410090  0.028615   \n",
       "55        0.349142  0.171010  0.499136    0.360110    0.406894  0.015924   \n",
       "56        0.263502  0.295546  0.599807    0.152410    0.283339  0.075191   \n",
       "57        0.373571  0.173789  0.474019    0.274102    0.383992  0.027800   \n",
       "\n",
       "dataset      mean  \n",
       "0        0.478349  \n",
       "1        0.477789  \n",
       "2        0.472369  \n",
       "3        0.471623  \n",
       "4        0.466021  \n",
       "5        0.465819  \n",
       "6        0.463548  \n",
       "7        0.462806  \n",
       "8        0.462745  \n",
       "9        0.460839  \n",
       "10       0.460035  \n",
       "11       0.457948  \n",
       "12       0.454741  \n",
       "13       0.453481  \n",
       "14       0.449082  \n",
       "15       0.448889  \n",
       "16       0.447052  \n",
       "17       0.445001  \n",
       "18       0.443051  \n",
       "19       0.437188  \n",
       "20       0.434534  \n",
       "21       0.433200  \n",
       "22       0.432172  \n",
       "23       0.429187  \n",
       "24       0.428921  \n",
       "25       0.428402  \n",
       "26       0.426488  \n",
       "27       0.424124  \n",
       "28       0.423218  \n",
       "29       0.422185  \n",
       "30       0.416643  \n",
       "31       0.415162  \n",
       "32       0.411739  \n",
       "33       0.403382  \n",
       "34       0.397947  \n",
       "35       0.386879  \n",
       "36       0.384456  \n",
       "37       0.384134  \n",
       "38       0.381062  \n",
       "39       0.380701  \n",
       "40       0.380612  \n",
       "41       0.378327  \n",
       "42       0.377485  \n",
       "43       0.377363  \n",
       "44       0.374245  \n",
       "45       0.372411  \n",
       "46       0.371770  \n",
       "47       0.366141  \n",
       "48       0.365326  \n",
       "49       0.362790  \n",
       "50       0.338441  \n",
       "51       0.332078  \n",
       "52       0.328941  \n",
       "53       0.325501  \n",
       "54       0.309193  \n",
       "55       0.291157  \n",
       "56       0.288027  \n",
       "57       0.279593  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assume df has columns: [\"Method\", \"dataset\", \"prr\"]\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "tables = {}\n",
    "for model, df_model in df.groupby(\"model\"):\n",
    "    df_wide = df_model.pivot_table(\n",
    "        index=\"Method\", columns=\"dataset\", values=\"prr\", aggfunc=\"mean\"\n",
    "    )\n",
    "    df_wide[\"mean\"] = df_wide.mean(axis=1)\n",
    "    df_wide = df_wide.sort_values(\"mean\", ascending=False).reset_index()\n",
    "    tables[model] = df_wide\n",
    "    filename = f\"../resources/llm_resources/{model}_results.csv\"\n",
    "    df_wide.to_csv(filename, index=False)\n",
    "\n",
    "# Example: show the table for one model\n",
    "tables[\"llama8b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65916373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>Method</th>\n",
       "      <th>coqa_no_context</th>\n",
       "      <th>gsm8k_cot</th>\n",
       "      <th>mmlu</th>\n",
       "      <th>trivia</th>\n",
       "      <th>wmt14_fren</th>\n",
       "      <th>wmt19_deen</th>\n",
       "      <th>xsum</th>\n",
       "      <th>mean</th>\n",
       "      <th>coqa_no_context_rank</th>\n",
       "      <th>gsm8k_cot_rank</th>\n",
       "      <th>mmlu_rank</th>\n",
       "      <th>trivia_rank</th>\n",
       "      <th>wmt14_fren_rank</th>\n",
       "      <th>wmt19_deen_rank</th>\n",
       "      <th>xsum_rank</th>\n",
       "      <th>mean_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMBA_5_Exp_FeatureWise</td>\n",
       "      <td>0.353907</td>\n",
       "      <td>0.419594</td>\n",
       "      <td>0.484351</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.475661</td>\n",
       "      <td>0.612408</td>\n",
       "      <td>0.405422</td>\n",
       "      <td>0.478349</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMBA_5_Beta_FeatureWise</td>\n",
       "      <td>0.355931</td>\n",
       "      <td>0.409402</td>\n",
       "      <td>0.481497</td>\n",
       "      <td>0.599280</td>\n",
       "      <td>0.475688</td>\n",
       "      <td>0.613109</td>\n",
       "      <td>0.409612</td>\n",
       "      <td>0.477789</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Master_1_Exp_FeatureWise</td>\n",
       "      <td>0.356833</td>\n",
       "      <td>0.438419</td>\n",
       "      <td>0.461269</td>\n",
       "      <td>0.605990</td>\n",
       "      <td>0.470549</td>\n",
       "      <td>0.565129</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>0.472369</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Master_1_Beta_FeatureWise</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>0.430522</td>\n",
       "      <td>0.460514</td>\n",
       "      <td>0.606566</td>\n",
       "      <td>0.470590</td>\n",
       "      <td>0.564865</td>\n",
       "      <td>0.410731</td>\n",
       "      <td>0.471623</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Master_1_Exp_Global</td>\n",
       "      <td>0.368312</td>\n",
       "      <td>0.366505</td>\n",
       "      <td>0.463711</td>\n",
       "      <td>0.608272</td>\n",
       "      <td>0.459690</td>\n",
       "      <td>0.595384</td>\n",
       "      <td>0.400275</td>\n",
       "      <td>0.466021</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Master_1_Beta_Global</td>\n",
       "      <td>0.368916</td>\n",
       "      <td>0.365808</td>\n",
       "      <td>0.463258</td>\n",
       "      <td>0.608094</td>\n",
       "      <td>0.458775</td>\n",
       "      <td>0.595235</td>\n",
       "      <td>0.400645</td>\n",
       "      <td>0.465819</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GreedySemanticEnrichedMaxprobAveDissimilarity</td>\n",
       "      <td>0.366889</td>\n",
       "      <td>0.362325</td>\n",
       "      <td>0.491508</td>\n",
       "      <td>0.602602</td>\n",
       "      <td>0.442589</td>\n",
       "      <td>0.584028</td>\n",
       "      <td>0.394897</td>\n",
       "      <td>0.463548</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IMBA_2_Beta_Global</td>\n",
       "      <td>0.376108</td>\n",
       "      <td>0.368005</td>\n",
       "      <td>0.461837</td>\n",
       "      <td>0.614290</td>\n",
       "      <td>0.440997</td>\n",
       "      <td>0.577845</td>\n",
       "      <td>0.400559</td>\n",
       "      <td>0.462806</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IMBA_2_Exp_Global</td>\n",
       "      <td>0.377028</td>\n",
       "      <td>0.371017</td>\n",
       "      <td>0.461118</td>\n",
       "      <td>0.616403</td>\n",
       "      <td>0.438409</td>\n",
       "      <td>0.573779</td>\n",
       "      <td>0.401458</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IMBA_4_Exp_FeatureWise</td>\n",
       "      <td>0.379458</td>\n",
       "      <td>0.430302</td>\n",
       "      <td>0.478185</td>\n",
       "      <td>0.619169</td>\n",
       "      <td>0.344328</td>\n",
       "      <td>0.563956</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>0.460839</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CoCoA_like_3_Beta_FeatureWise</td>\n",
       "      <td>0.336145</td>\n",
       "      <td>0.409820</td>\n",
       "      <td>0.475266</td>\n",
       "      <td>0.594859</td>\n",
       "      <td>0.439068</td>\n",
       "      <td>0.576805</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>0.460035</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IMBA_4_Beta_FeatureWise</td>\n",
       "      <td>0.379941</td>\n",
       "      <td>0.419805</td>\n",
       "      <td>0.476333</td>\n",
       "      <td>0.619484</td>\n",
       "      <td>0.335432</td>\n",
       "      <td>0.559856</td>\n",
       "      <td>0.414785</td>\n",
       "      <td>0.457948</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CoCoA_like_3_Exp_FeatureWise</td>\n",
       "      <td>0.339954</td>\n",
       "      <td>0.419487</td>\n",
       "      <td>0.478852</td>\n",
       "      <td>0.591325</td>\n",
       "      <td>0.441541</td>\n",
       "      <td>0.576132</td>\n",
       "      <td>0.335898</td>\n",
       "      <td>0.454741</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GreedySemanticEnrichedPPLAveDissimilarity</td>\n",
       "      <td>0.351629</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>0.457640</td>\n",
       "      <td>0.597873</td>\n",
       "      <td>0.439909</td>\n",
       "      <td>0.508992</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.453481</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IMBA_2_Exp_FeatureWise</td>\n",
       "      <td>0.382852</td>\n",
       "      <td>0.435330</td>\n",
       "      <td>0.455928</td>\n",
       "      <td>0.625140</td>\n",
       "      <td>0.319405</td>\n",
       "      <td>0.518174</td>\n",
       "      <td>0.406742</td>\n",
       "      <td>0.449082</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                                         Method  coqa_no_context  \\\n",
       "0                               IMBA_5_Exp_FeatureWise         0.353907   \n",
       "1                              IMBA_5_Beta_FeatureWise         0.355931   \n",
       "2                             Master_1_Exp_FeatureWise         0.356833   \n",
       "3                            Master_1_Beta_FeatureWise         0.357576   \n",
       "4                                  Master_1_Exp_Global         0.368312   \n",
       "5                                 Master_1_Beta_Global         0.368916   \n",
       "6        GreedySemanticEnrichedMaxprobAveDissimilarity         0.366889   \n",
       "7                                   IMBA_2_Beta_Global         0.376108   \n",
       "8                                    IMBA_2_Exp_Global         0.377028   \n",
       "9                               IMBA_4_Exp_FeatureWise         0.379458   \n",
       "10                       CoCoA_like_3_Beta_FeatureWise         0.336145   \n",
       "11                             IMBA_4_Beta_FeatureWise         0.379941   \n",
       "12                        CoCoA_like_3_Exp_FeatureWise         0.339954   \n",
       "13           GreedySemanticEnrichedPPLAveDissimilarity         0.351629   \n",
       "14                              IMBA_2_Exp_FeatureWise         0.382852   \n",
       "\n",
       "dataset  gsm8k_cot      mmlu    trivia  wmt14_fren  wmt19_deen      xsum  \\\n",
       "0         0.419594  0.484351  0.597100    0.475661    0.612408  0.405422   \n",
       "1         0.409402  0.481497  0.599280    0.475688    0.613109  0.409612   \n",
       "2         0.438419  0.461269  0.605990    0.470549    0.565129  0.408392   \n",
       "3         0.430522  0.460514  0.606566    0.470590    0.564865  0.410731   \n",
       "4         0.366505  0.463711  0.608272    0.459690    0.595384  0.400275   \n",
       "5         0.365808  0.463258  0.608094    0.458775    0.595235  0.400645   \n",
       "6         0.362325  0.491508  0.602602    0.442589    0.584028  0.394897   \n",
       "7         0.368005  0.461837  0.614290    0.440997    0.577845  0.400559   \n",
       "8         0.371017  0.461118  0.616403    0.438409    0.573779  0.401458   \n",
       "9         0.430302  0.478185  0.619169    0.344328    0.563956  0.410475   \n",
       "10        0.409820  0.475266  0.594859    0.439068    0.576805  0.388284   \n",
       "11        0.419805  0.476333  0.619484    0.335432    0.559856  0.414785   \n",
       "12        0.419487  0.478852  0.591325    0.441541    0.576132  0.335898   \n",
       "13        0.417121  0.457640  0.597873    0.439909    0.508992  0.401200   \n",
       "14        0.435330  0.455928  0.625140    0.319405    0.518174  0.406742   \n",
       "\n",
       "dataset      mean  coqa_no_context_rank  gsm8k_cot_rank  mmlu_rank  \\\n",
       "0        0.478349                    28              10          9   \n",
       "1        0.477789                    27              15         11   \n",
       "2        0.472369                    23               1         28   \n",
       "3        0.471623                    22               4         30   \n",
       "4        0.466021                    18              25         24   \n",
       "5        0.465819                    16              26         25   \n",
       "6        0.463548                    19              28          3   \n",
       "7        0.462806                     8              23         27   \n",
       "8        0.462745                     7              22         29   \n",
       "9        0.460839                     5               5         14   \n",
       "10       0.460035                    43              14         18   \n",
       "11       0.457948                     4               9         17   \n",
       "12       0.454741                    42              11         13   \n",
       "13       0.453481                    31              12         36   \n",
       "14       0.449082                     2               2         39   \n",
       "\n",
       "dataset  trivia_rank  wmt14_fren_rank  wmt19_deen_rank  xsum_rank  mean_rank  \n",
       "0                 35                2                2          8          1  \n",
       "1                 30                1                1          5          2  \n",
       "2                 19                4               19          6          3  \n",
       "3                 18                3               20          2          4  \n",
       "4                 15                5                5         13          5  \n",
       "5                 16                6                6         11          6  \n",
       "6                 25                9                9         14          7  \n",
       "7                 14               11               12         12          8  \n",
       "8                 10               15               15          9          9  \n",
       "9                  8               39               21          3         10  \n",
       "10                36               13               13         16         11  \n",
       "11                 7               43               23          1         12  \n",
       "12                40               10               14         26         13  \n",
       "13                31               12               40         10         14  \n",
       "14                 2               46               33          7         15  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assume df has columns: [\"model\", \"Method\", \"dataset\", \"prr\"]\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "tables = {}\n",
    "for model, df_model in df.groupby(\"model\"):\n",
    "    # pivot to wide form\n",
    "    df_wide = df_model.pivot_table(\n",
    "        index=\"Method\", columns=\"dataset\", values=\"prr\", aggfunc=\"mean\"\n",
    "    )\n",
    "\n",
    "    # add mean across datasets\n",
    "    df_wide[\"mean\"] = df_wide.mean(axis=1)\n",
    "\n",
    "    # compute ranking for each dataset (higher prr = better rank)\n",
    "    for col in df_wide.columns:\n",
    "        if col != \"mean\":\n",
    "            df_wide[f\"{col}_rank\"] = df_wide[col].rank(\n",
    "                method=\"min\", ascending=False\n",
    "            ).astype(int)\n",
    "\n",
    "    # also rank by mean\n",
    "    df_wide[\"mean_rank\"] = df_wide[\"mean\"].rank(\n",
    "        method=\"min\", ascending=False\n",
    "    ).astype(int)\n",
    "\n",
    "    # sort by mean rank\n",
    "    df_wide = df_wide.sort_values(\"mean\", ascending=False).reset_index()\n",
    "\n",
    "    tables[model] = df_wide\n",
    "    filename = f\"../resources/llm_resources/{model}_results.csv\"\n",
    "    df_wide.to_csv(filename, index=False)\n",
    "\n",
    "# Example: show the table for one model\n",
    "tables[\"llama8b\"][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbc721a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"2\" halign=\"left\">coqa_no_context</th>\n",
       "      <th colspan=\"2\" halign=\"left\">gsm8k_cot</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mmlu</th>\n",
       "      <th colspan=\"2\" halign=\"left\">trivia</th>\n",
       "      <th colspan=\"2\" halign=\"left\">wmt14_fren</th>\n",
       "      <th colspan=\"2\" halign=\"left\">wmt19_deen</th>\n",
       "      <th colspan=\"2\" halign=\"left\">xsum</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMBA_5_Exp_FeatureWise</td>\n",
       "      <td>0.353907</td>\n",
       "      <td>28</td>\n",
       "      <td>0.419594</td>\n",
       "      <td>10</td>\n",
       "      <td>0.484351</td>\n",
       "      <td>9</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>35</td>\n",
       "      <td>0.475661</td>\n",
       "      <td>2</td>\n",
       "      <td>0.612408</td>\n",
       "      <td>2</td>\n",
       "      <td>0.405422</td>\n",
       "      <td>8</td>\n",
       "      <td>0.478349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMBA_5_Beta_FeatureWise</td>\n",
       "      <td>0.355931</td>\n",
       "      <td>27</td>\n",
       "      <td>0.409402</td>\n",
       "      <td>15</td>\n",
       "      <td>0.481497</td>\n",
       "      <td>11</td>\n",
       "      <td>0.599280</td>\n",
       "      <td>30</td>\n",
       "      <td>0.475688</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613109</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409612</td>\n",
       "      <td>5</td>\n",
       "      <td>0.477789</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Master_1_Exp_FeatureWise</td>\n",
       "      <td>0.356833</td>\n",
       "      <td>23</td>\n",
       "      <td>0.438419</td>\n",
       "      <td>1</td>\n",
       "      <td>0.461269</td>\n",
       "      <td>28</td>\n",
       "      <td>0.605990</td>\n",
       "      <td>19</td>\n",
       "      <td>0.470549</td>\n",
       "      <td>4</td>\n",
       "      <td>0.565129</td>\n",
       "      <td>19</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>6</td>\n",
       "      <td>0.472369</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Master_1_Beta_FeatureWise</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>22</td>\n",
       "      <td>0.430522</td>\n",
       "      <td>4</td>\n",
       "      <td>0.460514</td>\n",
       "      <td>30</td>\n",
       "      <td>0.606566</td>\n",
       "      <td>18</td>\n",
       "      <td>0.470590</td>\n",
       "      <td>3</td>\n",
       "      <td>0.564865</td>\n",
       "      <td>20</td>\n",
       "      <td>0.410731</td>\n",
       "      <td>2</td>\n",
       "      <td>0.471623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Master_1_Exp_Global</td>\n",
       "      <td>0.368312</td>\n",
       "      <td>18</td>\n",
       "      <td>0.366505</td>\n",
       "      <td>25</td>\n",
       "      <td>0.463711</td>\n",
       "      <td>24</td>\n",
       "      <td>0.608272</td>\n",
       "      <td>15</td>\n",
       "      <td>0.459690</td>\n",
       "      <td>5</td>\n",
       "      <td>0.595384</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400275</td>\n",
       "      <td>13</td>\n",
       "      <td>0.466021</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Master_1_Beta_Global</td>\n",
       "      <td>0.368916</td>\n",
       "      <td>16</td>\n",
       "      <td>0.365808</td>\n",
       "      <td>26</td>\n",
       "      <td>0.463258</td>\n",
       "      <td>25</td>\n",
       "      <td>0.608094</td>\n",
       "      <td>16</td>\n",
       "      <td>0.458775</td>\n",
       "      <td>6</td>\n",
       "      <td>0.595235</td>\n",
       "      <td>6</td>\n",
       "      <td>0.400645</td>\n",
       "      <td>11</td>\n",
       "      <td>0.465819</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GreedySemanticEnrichedMaxprobAveDissimilarity</td>\n",
       "      <td>0.366889</td>\n",
       "      <td>19</td>\n",
       "      <td>0.362325</td>\n",
       "      <td>28</td>\n",
       "      <td>0.491508</td>\n",
       "      <td>3</td>\n",
       "      <td>0.602602</td>\n",
       "      <td>25</td>\n",
       "      <td>0.442589</td>\n",
       "      <td>9</td>\n",
       "      <td>0.584028</td>\n",
       "      <td>9</td>\n",
       "      <td>0.394897</td>\n",
       "      <td>14</td>\n",
       "      <td>0.463548</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IMBA_2_Beta_Global</td>\n",
       "      <td>0.376108</td>\n",
       "      <td>8</td>\n",
       "      <td>0.368005</td>\n",
       "      <td>23</td>\n",
       "      <td>0.461837</td>\n",
       "      <td>27</td>\n",
       "      <td>0.614290</td>\n",
       "      <td>14</td>\n",
       "      <td>0.440997</td>\n",
       "      <td>11</td>\n",
       "      <td>0.577845</td>\n",
       "      <td>12</td>\n",
       "      <td>0.400559</td>\n",
       "      <td>12</td>\n",
       "      <td>0.462806</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IMBA_2_Exp_Global</td>\n",
       "      <td>0.377028</td>\n",
       "      <td>7</td>\n",
       "      <td>0.371017</td>\n",
       "      <td>22</td>\n",
       "      <td>0.461118</td>\n",
       "      <td>29</td>\n",
       "      <td>0.616403</td>\n",
       "      <td>10</td>\n",
       "      <td>0.438409</td>\n",
       "      <td>15</td>\n",
       "      <td>0.573779</td>\n",
       "      <td>15</td>\n",
       "      <td>0.401458</td>\n",
       "      <td>9</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IMBA_4_Exp_FeatureWise</td>\n",
       "      <td>0.379458</td>\n",
       "      <td>5</td>\n",
       "      <td>0.430302</td>\n",
       "      <td>5</td>\n",
       "      <td>0.478185</td>\n",
       "      <td>14</td>\n",
       "      <td>0.619169</td>\n",
       "      <td>8</td>\n",
       "      <td>0.344328</td>\n",
       "      <td>39</td>\n",
       "      <td>0.563956</td>\n",
       "      <td>21</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>3</td>\n",
       "      <td>0.460839</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CoCoA_like_3_Beta_FeatureWise</td>\n",
       "      <td>0.336145</td>\n",
       "      <td>43</td>\n",
       "      <td>0.409820</td>\n",
       "      <td>14</td>\n",
       "      <td>0.475266</td>\n",
       "      <td>18</td>\n",
       "      <td>0.594859</td>\n",
       "      <td>36</td>\n",
       "      <td>0.439068</td>\n",
       "      <td>13</td>\n",
       "      <td>0.576805</td>\n",
       "      <td>13</td>\n",
       "      <td>0.388284</td>\n",
       "      <td>16</td>\n",
       "      <td>0.460035</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IMBA_4_Beta_FeatureWise</td>\n",
       "      <td>0.379941</td>\n",
       "      <td>4</td>\n",
       "      <td>0.419805</td>\n",
       "      <td>9</td>\n",
       "      <td>0.476333</td>\n",
       "      <td>17</td>\n",
       "      <td>0.619484</td>\n",
       "      <td>7</td>\n",
       "      <td>0.335432</td>\n",
       "      <td>43</td>\n",
       "      <td>0.559856</td>\n",
       "      <td>23</td>\n",
       "      <td>0.414785</td>\n",
       "      <td>1</td>\n",
       "      <td>0.457948</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CoCoA_like_3_Exp_FeatureWise</td>\n",
       "      <td>0.339954</td>\n",
       "      <td>42</td>\n",
       "      <td>0.419487</td>\n",
       "      <td>11</td>\n",
       "      <td>0.478852</td>\n",
       "      <td>13</td>\n",
       "      <td>0.591325</td>\n",
       "      <td>40</td>\n",
       "      <td>0.441541</td>\n",
       "      <td>10</td>\n",
       "      <td>0.576132</td>\n",
       "      <td>14</td>\n",
       "      <td>0.335898</td>\n",
       "      <td>26</td>\n",
       "      <td>0.454741</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GreedySemanticEnrichedPPLAveDissimilarity</td>\n",
       "      <td>0.351629</td>\n",
       "      <td>31</td>\n",
       "      <td>0.417121</td>\n",
       "      <td>12</td>\n",
       "      <td>0.457640</td>\n",
       "      <td>36</td>\n",
       "      <td>0.597873</td>\n",
       "      <td>31</td>\n",
       "      <td>0.439909</td>\n",
       "      <td>12</td>\n",
       "      <td>0.508992</td>\n",
       "      <td>40</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.453481</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IMBA_2_Exp_FeatureWise</td>\n",
       "      <td>0.382852</td>\n",
       "      <td>2</td>\n",
       "      <td>0.435330</td>\n",
       "      <td>2</td>\n",
       "      <td>0.455928</td>\n",
       "      <td>39</td>\n",
       "      <td>0.625140</td>\n",
       "      <td>2</td>\n",
       "      <td>0.319405</td>\n",
       "      <td>46</td>\n",
       "      <td>0.518174</td>\n",
       "      <td>33</td>\n",
       "      <td>0.406742</td>\n",
       "      <td>7</td>\n",
       "      <td>0.449082</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IMBA_2_Beta_FeatureWise</td>\n",
       "      <td>0.380607</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428647</td>\n",
       "      <td>6</td>\n",
       "      <td>0.457606</td>\n",
       "      <td>37</td>\n",
       "      <td>0.623677</td>\n",
       "      <td>6</td>\n",
       "      <td>0.318293</td>\n",
       "      <td>47</td>\n",
       "      <td>0.523291</td>\n",
       "      <td>32</td>\n",
       "      <td>0.410099</td>\n",
       "      <td>4</td>\n",
       "      <td>0.448889</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GreedySemanticEnrichedMTEAveDissimilarity</td>\n",
       "      <td>0.350511</td>\n",
       "      <td>33</td>\n",
       "      <td>0.433381</td>\n",
       "      <td>3</td>\n",
       "      <td>0.408473</td>\n",
       "      <td>49</td>\n",
       "      <td>0.603853</td>\n",
       "      <td>24</td>\n",
       "      <td>0.435925</td>\n",
       "      <td>16</td>\n",
       "      <td>0.505174</td>\n",
       "      <td>42</td>\n",
       "      <td>0.392047</td>\n",
       "      <td>15</td>\n",
       "      <td>0.447052</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Master_3_Beta_FeatureWise</td>\n",
       "      <td>0.356386</td>\n",
       "      <td>25</td>\n",
       "      <td>0.411995</td>\n",
       "      <td>13</td>\n",
       "      <td>0.455742</td>\n",
       "      <td>40</td>\n",
       "      <td>0.601985</td>\n",
       "      <td>26</td>\n",
       "      <td>0.450759</td>\n",
       "      <td>8</td>\n",
       "      <td>0.582538</td>\n",
       "      <td>10</td>\n",
       "      <td>0.255601</td>\n",
       "      <td>28</td>\n",
       "      <td>0.445001</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Master_3_Exp_FeatureWise</td>\n",
       "      <td>0.356415</td>\n",
       "      <td>24</td>\n",
       "      <td>0.408491</td>\n",
       "      <td>16</td>\n",
       "      <td>0.456057</td>\n",
       "      <td>38</td>\n",
       "      <td>0.600984</td>\n",
       "      <td>27</td>\n",
       "      <td>0.450969</td>\n",
       "      <td>7</td>\n",
       "      <td>0.580766</td>\n",
       "      <td>11</td>\n",
       "      <td>0.247672</td>\n",
       "      <td>30</td>\n",
       "      <td>0.443051</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CoCoA_like_1_Beta_FeatureWise</td>\n",
       "      <td>0.340199</td>\n",
       "      <td>40</td>\n",
       "      <td>0.395301</td>\n",
       "      <td>17</td>\n",
       "      <td>0.472162</td>\n",
       "      <td>20</td>\n",
       "      <td>0.592579</td>\n",
       "      <td>38</td>\n",
       "      <td>0.438466</td>\n",
       "      <td>14</td>\n",
       "      <td>0.611143</td>\n",
       "      <td>3</td>\n",
       "      <td>0.210464</td>\n",
       "      <td>34</td>\n",
       "      <td>0.437188</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CoCoA_like_1_Exp_FeatureWise</td>\n",
       "      <td>0.340182</td>\n",
       "      <td>41</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>18</td>\n",
       "      <td>0.476597</td>\n",
       "      <td>16</td>\n",
       "      <td>0.591501</td>\n",
       "      <td>39</td>\n",
       "      <td>0.433983</td>\n",
       "      <td>17</td>\n",
       "      <td>0.602976</td>\n",
       "      <td>4</td>\n",
       "      <td>0.202109</td>\n",
       "      <td>37</td>\n",
       "      <td>0.434534</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IMBA_4_Exp_Global</td>\n",
       "      <td>0.350788</td>\n",
       "      <td>32</td>\n",
       "      <td>0.338917</td>\n",
       "      <td>33</td>\n",
       "      <td>0.489219</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600092</td>\n",
       "      <td>28</td>\n",
       "      <td>0.354359</td>\n",
       "      <td>34</td>\n",
       "      <td>0.524928</td>\n",
       "      <td>31</td>\n",
       "      <td>0.374095</td>\n",
       "      <td>18</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Master_3_Exp_Global</td>\n",
       "      <td>0.362935</td>\n",
       "      <td>21</td>\n",
       "      <td>0.354143</td>\n",
       "      <td>29</td>\n",
       "      <td>0.459075</td>\n",
       "      <td>34</td>\n",
       "      <td>0.604965</td>\n",
       "      <td>22</td>\n",
       "      <td>0.431890</td>\n",
       "      <td>18</td>\n",
       "      <td>0.594252</td>\n",
       "      <td>7</td>\n",
       "      <td>0.217942</td>\n",
       "      <td>32</td>\n",
       "      <td>0.432172</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>IMBA_4_Beta_Global</td>\n",
       "      <td>0.348072</td>\n",
       "      <td>34</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>35</td>\n",
       "      <td>0.487914</td>\n",
       "      <td>7</td>\n",
       "      <td>0.597838</td>\n",
       "      <td>32</td>\n",
       "      <td>0.349689</td>\n",
       "      <td>36</td>\n",
       "      <td>0.515189</td>\n",
       "      <td>37</td>\n",
       "      <td>0.373197</td>\n",
       "      <td>20</td>\n",
       "      <td>0.429187</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Master_3_Beta_Global</td>\n",
       "      <td>0.364434</td>\n",
       "      <td>20</td>\n",
       "      <td>0.351208</td>\n",
       "      <td>30</td>\n",
       "      <td>0.459089</td>\n",
       "      <td>33</td>\n",
       "      <td>0.605563</td>\n",
       "      <td>21</td>\n",
       "      <td>0.423418</td>\n",
       "      <td>19</td>\n",
       "      <td>0.591117</td>\n",
       "      <td>8</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>36</td>\n",
       "      <td>0.428921</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IMBA_5_Exp_Global</td>\n",
       "      <td>0.343138</td>\n",
       "      <td>38</td>\n",
       "      <td>0.333372</td>\n",
       "      <td>34</td>\n",
       "      <td>0.492882</td>\n",
       "      <td>2</td>\n",
       "      <td>0.589422</td>\n",
       "      <td>43</td>\n",
       "      <td>0.350708</td>\n",
       "      <td>35</td>\n",
       "      <td>0.515644</td>\n",
       "      <td>36</td>\n",
       "      <td>0.373645</td>\n",
       "      <td>19</td>\n",
       "      <td>0.428402</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IMBA_5_Beta_Global</td>\n",
       "      <td>0.341805</td>\n",
       "      <td>39</td>\n",
       "      <td>0.331868</td>\n",
       "      <td>36</td>\n",
       "      <td>0.491450</td>\n",
       "      <td>4</td>\n",
       "      <td>0.589069</td>\n",
       "      <td>44</td>\n",
       "      <td>0.347590</td>\n",
       "      <td>37</td>\n",
       "      <td>0.510569</td>\n",
       "      <td>39</td>\n",
       "      <td>0.373062</td>\n",
       "      <td>21</td>\n",
       "      <td>0.426488</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IMBA_3_Beta_FeatureWise</td>\n",
       "      <td>0.376074</td>\n",
       "      <td>9</td>\n",
       "      <td>0.419893</td>\n",
       "      <td>8</td>\n",
       "      <td>0.453696</td>\n",
       "      <td>42</td>\n",
       "      <td>0.623715</td>\n",
       "      <td>4</td>\n",
       "      <td>0.316124</td>\n",
       "      <td>48</td>\n",
       "      <td>0.517650</td>\n",
       "      <td>34</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>27</td>\n",
       "      <td>0.424124</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IMBA_3_Exp_FeatureWise</td>\n",
       "      <td>0.377913</td>\n",
       "      <td>6</td>\n",
       "      <td>0.426849</td>\n",
       "      <td>7</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>44</td>\n",
       "      <td>0.623697</td>\n",
       "      <td>5</td>\n",
       "      <td>0.315689</td>\n",
       "      <td>49</td>\n",
       "      <td>0.511032</td>\n",
       "      <td>38</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>29</td>\n",
       "      <td>0.423218</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>IMBA_3_Beta_Global</td>\n",
       "      <td>0.375223</td>\n",
       "      <td>11</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>31</td>\n",
       "      <td>0.453754</td>\n",
       "      <td>41</td>\n",
       "      <td>0.626566</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381189</td>\n",
       "      <td>27</td>\n",
       "      <td>0.558364</td>\n",
       "      <td>25</td>\n",
       "      <td>0.209408</td>\n",
       "      <td>35</td>\n",
       "      <td>0.422185</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>IMBA_3_Exp_Global</td>\n",
       "      <td>0.373297</td>\n",
       "      <td>13</td>\n",
       "      <td>0.363378</td>\n",
       "      <td>27</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>43</td>\n",
       "      <td>0.624717</td>\n",
       "      <td>3</td>\n",
       "      <td>0.360944</td>\n",
       "      <td>31</td>\n",
       "      <td>0.541079</td>\n",
       "      <td>29</td>\n",
       "      <td>0.199686</td>\n",
       "      <td>38</td>\n",
       "      <td>0.416643</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CoCoA_like_3_Exp_Global</td>\n",
       "      <td>0.314715</td>\n",
       "      <td>50</td>\n",
       "      <td>0.317123</td>\n",
       "      <td>40</td>\n",
       "      <td>0.488686</td>\n",
       "      <td>6</td>\n",
       "      <td>0.572646</td>\n",
       "      <td>49</td>\n",
       "      <td>0.344201</td>\n",
       "      <td>40</td>\n",
       "      <td>0.516505</td>\n",
       "      <td>35</td>\n",
       "      <td>0.352260</td>\n",
       "      <td>23</td>\n",
       "      <td>0.415162</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CoCoA_like_3_Beta_Global</td>\n",
       "      <td>0.315034</td>\n",
       "      <td>49</td>\n",
       "      <td>0.314538</td>\n",
       "      <td>41</td>\n",
       "      <td>0.486222</td>\n",
       "      <td>8</td>\n",
       "      <td>0.570157</td>\n",
       "      <td>50</td>\n",
       "      <td>0.338264</td>\n",
       "      <td>42</td>\n",
       "      <td>0.505950</td>\n",
       "      <td>41</td>\n",
       "      <td>0.352009</td>\n",
       "      <td>24</td>\n",
       "      <td>0.411739</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CoCoA_like_1_Exp_Global</td>\n",
       "      <td>0.321302</td>\n",
       "      <td>47</td>\n",
       "      <td>0.326569</td>\n",
       "      <td>37</td>\n",
       "      <td>0.480832</td>\n",
       "      <td>12</td>\n",
       "      <td>0.583840</td>\n",
       "      <td>45</td>\n",
       "      <td>0.343926</td>\n",
       "      <td>41</td>\n",
       "      <td>0.540315</td>\n",
       "      <td>30</td>\n",
       "      <td>0.226889</td>\n",
       "      <td>31</td>\n",
       "      <td>0.403382</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MaximumSequenceProbability</td>\n",
       "      <td>0.290338</td>\n",
       "      <td>53</td>\n",
       "      <td>0.309910</td>\n",
       "      <td>42</td>\n",
       "      <td>0.516374</td>\n",
       "      <td>1</td>\n",
       "      <td>0.538346</td>\n",
       "      <td>54</td>\n",
       "      <td>0.320281</td>\n",
       "      <td>45</td>\n",
       "      <td>0.468913</td>\n",
       "      <td>45</td>\n",
       "      <td>0.341465</td>\n",
       "      <td>25</td>\n",
       "      <td>0.397947</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CoCoA_like_1_Beta_Global</td>\n",
       "      <td>0.317172</td>\n",
       "      <td>48</td>\n",
       "      <td>0.307602</td>\n",
       "      <td>46</td>\n",
       "      <td>0.483145</td>\n",
       "      <td>10</td>\n",
       "      <td>0.574003</td>\n",
       "      <td>48</td>\n",
       "      <td>0.321527</td>\n",
       "      <td>44</td>\n",
       "      <td>0.494155</td>\n",
       "      <td>43</td>\n",
       "      <td>0.210551</td>\n",
       "      <td>33</td>\n",
       "      <td>0.386879</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Master_2_Beta_FeatureWise</td>\n",
       "      <td>0.346188</td>\n",
       "      <td>37</td>\n",
       "      <td>0.309646</td>\n",
       "      <td>43</td>\n",
       "      <td>0.443848</td>\n",
       "      <td>48</td>\n",
       "      <td>0.590745</td>\n",
       "      <td>41</td>\n",
       "      <td>0.415769</td>\n",
       "      <td>21</td>\n",
       "      <td>0.567580</td>\n",
       "      <td>17</td>\n",
       "      <td>0.017416</td>\n",
       "      <td>51</td>\n",
       "      <td>0.384456</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Master_2_Exp_FeatureWise</td>\n",
       "      <td>0.346868</td>\n",
       "      <td>35</td>\n",
       "      <td>0.308184</td>\n",
       "      <td>45</td>\n",
       "      <td>0.444368</td>\n",
       "      <td>47</td>\n",
       "      <td>0.590386</td>\n",
       "      <td>42</td>\n",
       "      <td>0.416865</td>\n",
       "      <td>20</td>\n",
       "      <td>0.565201</td>\n",
       "      <td>18</td>\n",
       "      <td>0.017066</td>\n",
       "      <td>52</td>\n",
       "      <td>0.384134</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IMBA_1_Exp_Global</td>\n",
       "      <td>0.370339</td>\n",
       "      <td>14</td>\n",
       "      <td>0.274232</td>\n",
       "      <td>54</td>\n",
       "      <td>0.467714</td>\n",
       "      <td>23</td>\n",
       "      <td>0.607525</td>\n",
       "      <td>17</td>\n",
       "      <td>0.374167</td>\n",
       "      <td>30</td>\n",
       "      <td>0.550068</td>\n",
       "      <td>28</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>48</td>\n",
       "      <td>0.381062</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>IMBA_1_Beta_Global</td>\n",
       "      <td>0.368736</td>\n",
       "      <td>17</td>\n",
       "      <td>0.271991</td>\n",
       "      <td>55</td>\n",
       "      <td>0.468239</td>\n",
       "      <td>22</td>\n",
       "      <td>0.605630</td>\n",
       "      <td>20</td>\n",
       "      <td>0.374637</td>\n",
       "      <td>29</td>\n",
       "      <td>0.551691</td>\n",
       "      <td>27</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>47</td>\n",
       "      <td>0.380701</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Perplexity</td>\n",
       "      <td>0.260974</td>\n",
       "      <td>55</td>\n",
       "      <td>0.283814</td>\n",
       "      <td>50</td>\n",
       "      <td>0.468947</td>\n",
       "      <td>21</td>\n",
       "      <td>0.519790</td>\n",
       "      <td>55</td>\n",
       "      <td>0.345569</td>\n",
       "      <td>38</td>\n",
       "      <td>0.402386</td>\n",
       "      <td>53</td>\n",
       "      <td>0.382807</td>\n",
       "      <td>17</td>\n",
       "      <td>0.380612</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Master_2_Exp_Global</td>\n",
       "      <td>0.353573</td>\n",
       "      <td>29</td>\n",
       "      <td>0.278936</td>\n",
       "      <td>52</td>\n",
       "      <td>0.449461</td>\n",
       "      <td>45</td>\n",
       "      <td>0.597258</td>\n",
       "      <td>33</td>\n",
       "      <td>0.388970</td>\n",
       "      <td>25</td>\n",
       "      <td>0.558942</td>\n",
       "      <td>24</td>\n",
       "      <td>0.021147</td>\n",
       "      <td>50</td>\n",
       "      <td>0.378327</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Master_2_Beta_Global</td>\n",
       "      <td>0.353239</td>\n",
       "      <td>30</td>\n",
       "      <td>0.275592</td>\n",
       "      <td>53</td>\n",
       "      <td>0.449185</td>\n",
       "      <td>46</td>\n",
       "      <td>0.597174</td>\n",
       "      <td>34</td>\n",
       "      <td>0.386715</td>\n",
       "      <td>26</td>\n",
       "      <td>0.558175</td>\n",
       "      <td>26</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>49</td>\n",
       "      <td>0.377485</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>CoCoA_like_2_Beta_FeatureWise</td>\n",
       "      <td>0.330824</td>\n",
       "      <td>45</td>\n",
       "      <td>0.297892</td>\n",
       "      <td>48</td>\n",
       "      <td>0.459112</td>\n",
       "      <td>32</td>\n",
       "      <td>0.581284</td>\n",
       "      <td>46</td>\n",
       "      <td>0.390238</td>\n",
       "      <td>24</td>\n",
       "      <td>0.570247</td>\n",
       "      <td>16</td>\n",
       "      <td>0.011944</td>\n",
       "      <td>57</td>\n",
       "      <td>0.377363</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>CoCoA_like_2_Exp_FeatureWise</td>\n",
       "      <td>0.333256</td>\n",
       "      <td>44</td>\n",
       "      <td>0.279902</td>\n",
       "      <td>51</td>\n",
       "      <td>0.462255</td>\n",
       "      <td>26</td>\n",
       "      <td>0.577841</td>\n",
       "      <td>47</td>\n",
       "      <td>0.391132</td>\n",
       "      <td>23</td>\n",
       "      <td>0.562686</td>\n",
       "      <td>22</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>56</td>\n",
       "      <td>0.374245</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SAR_t0.001</td>\n",
       "      <td>0.323909</td>\n",
       "      <td>46</td>\n",
       "      <td>0.389040</td>\n",
       "      <td>19</td>\n",
       "      <td>0.360104</td>\n",
       "      <td>52</td>\n",
       "      <td>0.592717</td>\n",
       "      <td>37</td>\n",
       "      <td>0.412148</td>\n",
       "      <td>22</td>\n",
       "      <td>0.472180</td>\n",
       "      <td>44</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>42</td>\n",
       "      <td>0.372411</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>GreedyAveDissimilarity</td>\n",
       "      <td>0.391002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366882</td>\n",
       "      <td>24</td>\n",
       "      <td>0.395135</td>\n",
       "      <td>50</td>\n",
       "      <td>0.615159</td>\n",
       "      <td>12</td>\n",
       "      <td>0.376409</td>\n",
       "      <td>28</td>\n",
       "      <td>0.450369</td>\n",
       "      <td>48</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>58</td>\n",
       "      <td>0.371770</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IMBA_1_Beta_FeatureWise</td>\n",
       "      <td>0.374696</td>\n",
       "      <td>12</td>\n",
       "      <td>0.324490</td>\n",
       "      <td>38</td>\n",
       "      <td>0.460181</td>\n",
       "      <td>31</td>\n",
       "      <td>0.614801</td>\n",
       "      <td>13</td>\n",
       "      <td>0.287036</td>\n",
       "      <td>53</td>\n",
       "      <td>0.461238</td>\n",
       "      <td>46</td>\n",
       "      <td>0.040546</td>\n",
       "      <td>43</td>\n",
       "      <td>0.366141</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>IMBA_1_Exp_FeatureWise</td>\n",
       "      <td>0.376030</td>\n",
       "      <td>10</td>\n",
       "      <td>0.322517</td>\n",
       "      <td>39</td>\n",
       "      <td>0.457755</td>\n",
       "      <td>35</td>\n",
       "      <td>0.616460</td>\n",
       "      <td>9</td>\n",
       "      <td>0.288181</td>\n",
       "      <td>52</td>\n",
       "      <td>0.458103</td>\n",
       "      <td>47</td>\n",
       "      <td>0.038234</td>\n",
       "      <td>44</td>\n",
       "      <td>0.365326</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MeanTokenEntropy</td>\n",
       "      <td>0.247590</td>\n",
       "      <td>57</td>\n",
       "      <td>0.308429</td>\n",
       "      <td>44</td>\n",
       "      <td>0.362241</td>\n",
       "      <td>51</td>\n",
       "      <td>0.505860</td>\n",
       "      <td>56</td>\n",
       "      <td>0.357613</td>\n",
       "      <td>33</td>\n",
       "      <td>0.390544</td>\n",
       "      <td>55</td>\n",
       "      <td>0.367250</td>\n",
       "      <td>22</td>\n",
       "      <td>0.362790</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>CEDegMat</td>\n",
       "      <td>0.356052</td>\n",
       "      <td>26</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>47</td>\n",
       "      <td>0.335752</td>\n",
       "      <td>54</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>23</td>\n",
       "      <td>0.311105</td>\n",
       "      <td>50</td>\n",
       "      <td>0.395779</td>\n",
       "      <td>54</td>\n",
       "      <td>0.067962</td>\n",
       "      <td>41</td>\n",
       "      <td>0.338441</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>CoCoA_like_2_Exp_Global</td>\n",
       "      <td>0.302724</td>\n",
       "      <td>52</td>\n",
       "      <td>0.235115</td>\n",
       "      <td>57</td>\n",
       "      <td>0.477230</td>\n",
       "      <td>15</td>\n",
       "      <td>0.562359</td>\n",
       "      <td>51</td>\n",
       "      <td>0.289802</td>\n",
       "      <td>51</td>\n",
       "      <td>0.444612</td>\n",
       "      <td>49</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>55</td>\n",
       "      <td>0.332078</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>CoCoA_like_2_Beta_Global</td>\n",
       "      <td>0.305148</td>\n",
       "      <td>51</td>\n",
       "      <td>0.227752</td>\n",
       "      <td>58</td>\n",
       "      <td>0.474705</td>\n",
       "      <td>19</td>\n",
       "      <td>0.562225</td>\n",
       "      <td>52</td>\n",
       "      <td>0.283805</td>\n",
       "      <td>54</td>\n",
       "      <td>0.435249</td>\n",
       "      <td>50</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>54</td>\n",
       "      <td>0.328941</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>DegMat_NLI_score_entail</td>\n",
       "      <td>0.369725</td>\n",
       "      <td>15</td>\n",
       "      <td>0.294448</td>\n",
       "      <td>49</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>53</td>\n",
       "      <td>0.615934</td>\n",
       "      <td>11</td>\n",
       "      <td>0.220236</td>\n",
       "      <td>57</td>\n",
       "      <td>0.356593</td>\n",
       "      <td>57</td>\n",
       "      <td>0.075562</td>\n",
       "      <td>39</td>\n",
       "      <td>0.325501</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SemanticEntropy</td>\n",
       "      <td>0.289091</td>\n",
       "      <td>54</td>\n",
       "      <td>0.383727</td>\n",
       "      <td>20</td>\n",
       "      <td>0.234864</td>\n",
       "      <td>56</td>\n",
       "      <td>0.540778</td>\n",
       "      <td>53</td>\n",
       "      <td>0.277182</td>\n",
       "      <td>55</td>\n",
       "      <td>0.410090</td>\n",
       "      <td>51</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>45</td>\n",
       "      <td>0.309193</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>MonteCarloNormalizedSequenceEntropy</td>\n",
       "      <td>0.235884</td>\n",
       "      <td>58</td>\n",
       "      <td>0.349142</td>\n",
       "      <td>32</td>\n",
       "      <td>0.171010</td>\n",
       "      <td>58</td>\n",
       "      <td>0.499136</td>\n",
       "      <td>57</td>\n",
       "      <td>0.360110</td>\n",
       "      <td>32</td>\n",
       "      <td>0.406894</td>\n",
       "      <td>52</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>53</td>\n",
       "      <td>0.291157</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>EigValLaplacian_NLI_score_entail</td>\n",
       "      <td>0.346395</td>\n",
       "      <td>36</td>\n",
       "      <td>0.263502</td>\n",
       "      <td>56</td>\n",
       "      <td>0.295546</td>\n",
       "      <td>55</td>\n",
       "      <td>0.599807</td>\n",
       "      <td>29</td>\n",
       "      <td>0.152410</td>\n",
       "      <td>58</td>\n",
       "      <td>0.283339</td>\n",
       "      <td>58</td>\n",
       "      <td>0.075191</td>\n",
       "      <td>40</td>\n",
       "      <td>0.288027</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>MonteCarloSequenceEntropy</td>\n",
       "      <td>0.249878</td>\n",
       "      <td>56</td>\n",
       "      <td>0.373571</td>\n",
       "      <td>21</td>\n",
       "      <td>0.173789</td>\n",
       "      <td>57</td>\n",
       "      <td>0.474019</td>\n",
       "      <td>58</td>\n",
       "      <td>0.274102</td>\n",
       "      <td>56</td>\n",
       "      <td>0.383992</td>\n",
       "      <td>56</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>46</td>\n",
       "      <td>0.279593</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Method coqa_no_context       \\\n",
       "                                                            score rank   \n",
       "0                          IMBA_5_Exp_FeatureWise        0.353907   28   \n",
       "1                         IMBA_5_Beta_FeatureWise        0.355931   27   \n",
       "2                        Master_1_Exp_FeatureWise        0.356833   23   \n",
       "3                       Master_1_Beta_FeatureWise        0.357576   22   \n",
       "4                             Master_1_Exp_Global        0.368312   18   \n",
       "5                            Master_1_Beta_Global        0.368916   16   \n",
       "6   GreedySemanticEnrichedMaxprobAveDissimilarity        0.366889   19   \n",
       "7                              IMBA_2_Beta_Global        0.376108    8   \n",
       "8                               IMBA_2_Exp_Global        0.377028    7   \n",
       "9                          IMBA_4_Exp_FeatureWise        0.379458    5   \n",
       "10                  CoCoA_like_3_Beta_FeatureWise        0.336145   43   \n",
       "11                        IMBA_4_Beta_FeatureWise        0.379941    4   \n",
       "12                   CoCoA_like_3_Exp_FeatureWise        0.339954   42   \n",
       "13      GreedySemanticEnrichedPPLAveDissimilarity        0.351629   31   \n",
       "14                         IMBA_2_Exp_FeatureWise        0.382852    2   \n",
       "15                        IMBA_2_Beta_FeatureWise        0.380607    3   \n",
       "16      GreedySemanticEnrichedMTEAveDissimilarity        0.350511   33   \n",
       "17                      Master_3_Beta_FeatureWise        0.356386   25   \n",
       "18                       Master_3_Exp_FeatureWise        0.356415   24   \n",
       "19                  CoCoA_like_1_Beta_FeatureWise        0.340199   40   \n",
       "20                   CoCoA_like_1_Exp_FeatureWise        0.340182   41   \n",
       "21                              IMBA_4_Exp_Global        0.350788   32   \n",
       "22                            Master_3_Exp_Global        0.362935   21   \n",
       "23                             IMBA_4_Beta_Global        0.348072   34   \n",
       "24                           Master_3_Beta_Global        0.364434   20   \n",
       "25                              IMBA_5_Exp_Global        0.343138   38   \n",
       "26                             IMBA_5_Beta_Global        0.341805   39   \n",
       "27                        IMBA_3_Beta_FeatureWise        0.376074    9   \n",
       "28                         IMBA_3_Exp_FeatureWise        0.377913    6   \n",
       "29                             IMBA_3_Beta_Global        0.375223   11   \n",
       "30                              IMBA_3_Exp_Global        0.373297   13   \n",
       "31                        CoCoA_like_3_Exp_Global        0.314715   50   \n",
       "32                       CoCoA_like_3_Beta_Global        0.315034   49   \n",
       "33                        CoCoA_like_1_Exp_Global        0.321302   47   \n",
       "34                     MaximumSequenceProbability        0.290338   53   \n",
       "35                       CoCoA_like_1_Beta_Global        0.317172   48   \n",
       "36                      Master_2_Beta_FeatureWise        0.346188   37   \n",
       "37                       Master_2_Exp_FeatureWise        0.346868   35   \n",
       "38                              IMBA_1_Exp_Global        0.370339   14   \n",
       "39                             IMBA_1_Beta_Global        0.368736   17   \n",
       "40                                     Perplexity        0.260974   55   \n",
       "41                            Master_2_Exp_Global        0.353573   29   \n",
       "42                           Master_2_Beta_Global        0.353239   30   \n",
       "43                  CoCoA_like_2_Beta_FeatureWise        0.330824   45   \n",
       "44                   CoCoA_like_2_Exp_FeatureWise        0.333256   44   \n",
       "45                                     SAR_t0.001        0.323909   46   \n",
       "46                         GreedyAveDissimilarity        0.391002    1   \n",
       "47                        IMBA_1_Beta_FeatureWise        0.374696   12   \n",
       "48                         IMBA_1_Exp_FeatureWise        0.376030   10   \n",
       "49                               MeanTokenEntropy        0.247590   57   \n",
       "50                                       CEDegMat        0.356052   26   \n",
       "51                        CoCoA_like_2_Exp_Global        0.302724   52   \n",
       "52                       CoCoA_like_2_Beta_Global        0.305148   51   \n",
       "53                        DegMat_NLI_score_entail        0.369725   15   \n",
       "54                                SemanticEntropy        0.289091   54   \n",
       "55            MonteCarloNormalizedSequenceEntropy        0.235884   58   \n",
       "56               EigValLaplacian_NLI_score_entail        0.346395   36   \n",
       "57                      MonteCarloSequenceEntropy        0.249878   56   \n",
       "\n",
       "   gsm8k_cot           mmlu         trivia      wmt14_fren      wmt19_deen  \\\n",
       "       score rank     score rank     score rank      score rank      score   \n",
       "0   0.419594   10  0.484351    9  0.597100   35   0.475661    2   0.612408   \n",
       "1   0.409402   15  0.481497   11  0.599280   30   0.475688    1   0.613109   \n",
       "2   0.438419    1  0.461269   28  0.605990   19   0.470549    4   0.565129   \n",
       "3   0.430522    4  0.460514   30  0.606566   18   0.470590    3   0.564865   \n",
       "4   0.366505   25  0.463711   24  0.608272   15   0.459690    5   0.595384   \n",
       "5   0.365808   26  0.463258   25  0.608094   16   0.458775    6   0.595235   \n",
       "6   0.362325   28  0.491508    3  0.602602   25   0.442589    9   0.584028   \n",
       "7   0.368005   23  0.461837   27  0.614290   14   0.440997   11   0.577845   \n",
       "8   0.371017   22  0.461118   29  0.616403   10   0.438409   15   0.573779   \n",
       "9   0.430302    5  0.478185   14  0.619169    8   0.344328   39   0.563956   \n",
       "10  0.409820   14  0.475266   18  0.594859   36   0.439068   13   0.576805   \n",
       "11  0.419805    9  0.476333   17  0.619484    7   0.335432   43   0.559856   \n",
       "12  0.419487   11  0.478852   13  0.591325   40   0.441541   10   0.576132   \n",
       "13  0.417121   12  0.457640   36  0.597873   31   0.439909   12   0.508992   \n",
       "14  0.435330    2  0.455928   39  0.625140    2   0.319405   46   0.518174   \n",
       "15  0.428647    6  0.457606   37  0.623677    6   0.318293   47   0.523291   \n",
       "16  0.433381    3  0.408473   49  0.603853   24   0.435925   16   0.505174   \n",
       "17  0.411995   13  0.455742   40  0.601985   26   0.450759    8   0.582538   \n",
       "18  0.408491   16  0.456057   38  0.600984   27   0.450969    7   0.580766   \n",
       "19  0.395301   17  0.472162   20  0.592579   38   0.438466   14   0.611143   \n",
       "20  0.394394   18  0.476597   16  0.591501   39   0.433983   17   0.602976   \n",
       "21  0.338917   33  0.489219    5  0.600092   28   0.354359   34   0.524928   \n",
       "22  0.354143   29  0.459075   34  0.604965   22   0.431890   18   0.594252   \n",
       "23  0.332410   35  0.487914    7  0.597838   32   0.349689   36   0.515189   \n",
       "24  0.351208   30  0.459089   33  0.605563   21   0.423418   19   0.591117   \n",
       "25  0.333372   34  0.492882    2  0.589422   43   0.350708   35   0.515644   \n",
       "26  0.331868   36  0.491450    4  0.589069   44   0.347590   37   0.510569   \n",
       "27  0.419893    8  0.453696   42  0.623715    4   0.316124   48   0.517650   \n",
       "28  0.426849    7  0.452142   44  0.623697    5   0.315689   49   0.511032   \n",
       "29  0.350789   31  0.453754   41  0.626566    1   0.381189   27   0.558364   \n",
       "30  0.363378   27  0.453400   43  0.624717    3   0.360944   31   0.541079   \n",
       "31  0.317123   40  0.488686    6  0.572646   49   0.344201   40   0.516505   \n",
       "32  0.314538   41  0.486222    8  0.570157   50   0.338264   42   0.505950   \n",
       "33  0.326569   37  0.480832   12  0.583840   45   0.343926   41   0.540315   \n",
       "34  0.309910   42  0.516374    1  0.538346   54   0.320281   45   0.468913   \n",
       "35  0.307602   46  0.483145   10  0.574003   48   0.321527   44   0.494155   \n",
       "36  0.309646   43  0.443848   48  0.590745   41   0.415769   21   0.567580   \n",
       "37  0.308184   45  0.444368   47  0.590386   42   0.416865   20   0.565201   \n",
       "38  0.274232   54  0.467714   23  0.607525   17   0.374167   30   0.550068   \n",
       "39  0.271991   55  0.468239   22  0.605630   20   0.374637   29   0.551691   \n",
       "40  0.283814   50  0.468947   21  0.519790   55   0.345569   38   0.402386   \n",
       "41  0.278936   52  0.449461   45  0.597258   33   0.388970   25   0.558942   \n",
       "42  0.275592   53  0.449185   46  0.597174   34   0.386715   26   0.558175   \n",
       "43  0.297892   48  0.459112   32  0.581284   46   0.390238   24   0.570247   \n",
       "44  0.279902   51  0.462255   26  0.577841   47   0.391132   23   0.562686   \n",
       "45  0.389040   19  0.360104   52  0.592717   37   0.412148   22   0.472180   \n",
       "46  0.366882   24  0.395135   50  0.615159   12   0.376409   28   0.450369   \n",
       "47  0.324490   38  0.460181   31  0.614801   13   0.287036   53   0.461238   \n",
       "48  0.322517   39  0.457755   35  0.616460    9   0.288181   52   0.458103   \n",
       "49  0.308429   44  0.362241   51  0.505860   56   0.357613   33   0.390544   \n",
       "50  0.298368   47  0.335752   54  0.604070   23   0.311105   50   0.395779   \n",
       "51  0.235115   57  0.477230   15  0.562359   51   0.289802   51   0.444612   \n",
       "52  0.227752   58  0.474705   19  0.562225   52   0.283805   54   0.435249   \n",
       "53  0.294448   49  0.346010   53  0.615934   11   0.220236   57   0.356593   \n",
       "54  0.383727   20  0.234864   56  0.540778   53   0.277182   55   0.410090   \n",
       "55  0.349142   32  0.171010   58  0.499136   57   0.360110   32   0.406894   \n",
       "56  0.263502   56  0.295546   55  0.599807   29   0.152410   58   0.283339   \n",
       "57  0.373571   21  0.173789   57  0.474019   58   0.274102   56   0.383992   \n",
       "\n",
       "             xsum           mean       \n",
       "   rank     score rank     score rank  \n",
       "0     2  0.405422    8  0.478349    1  \n",
       "1     1  0.409612    5  0.477789    2  \n",
       "2    19  0.408392    6  0.472369    3  \n",
       "3    20  0.410731    2  0.471623    4  \n",
       "4     5  0.400275   13  0.466021    5  \n",
       "5     6  0.400645   11  0.465819    6  \n",
       "6     9  0.394897   14  0.463548    7  \n",
       "7    12  0.400559   12  0.462806    8  \n",
       "8    15  0.401458    9  0.462745    9  \n",
       "9    21  0.410475    3  0.460839   10  \n",
       "10   13  0.388284   16  0.460035   11  \n",
       "11   23  0.414785    1  0.457948   12  \n",
       "12   14  0.335898   26  0.454741   13  \n",
       "13   40  0.401200   10  0.453481   14  \n",
       "14   33  0.406742    7  0.449082   15  \n",
       "15   32  0.410099    4  0.448889   16  \n",
       "16   42  0.392047   15  0.447052   17  \n",
       "17   10  0.255601   28  0.445001   18  \n",
       "18   11  0.247672   30  0.443051   19  \n",
       "19    3  0.210464   34  0.437188   20  \n",
       "20    4  0.202109   37  0.434534   21  \n",
       "21   31  0.374095   18  0.433200   22  \n",
       "22    7  0.217942   32  0.432172   23  \n",
       "23   37  0.373197   20  0.429187   24  \n",
       "24    8  0.207621   36  0.428921   25  \n",
       "25   36  0.373645   19  0.428402   26  \n",
       "26   39  0.373062   21  0.426488   27  \n",
       "27   34  0.261719   27  0.424124   28  \n",
       "28   38  0.255200   29  0.423218   29  \n",
       "29   25  0.209408   35  0.422185   30  \n",
       "30   29  0.199686   38  0.416643   31  \n",
       "31   35  0.352260   23  0.415162   32  \n",
       "32   41  0.352009   24  0.411739   33  \n",
       "33   30  0.226889   31  0.403382   34  \n",
       "34   45  0.341465   25  0.397947   35  \n",
       "35   43  0.210551   33  0.386879   36  \n",
       "36   17  0.017416   51  0.384456   37  \n",
       "37   18  0.017066   52  0.384134   38  \n",
       "38   28  0.023388   48  0.381062   39  \n",
       "39   27  0.023986   47  0.380701   40  \n",
       "40   53  0.382807   17  0.380612   41  \n",
       "41   24  0.021147   50  0.378327   42  \n",
       "42   26  0.022315   49  0.377485   43  \n",
       "43   16  0.011944   57  0.377363   44  \n",
       "44   22  0.012644   56  0.374245   45  \n",
       "45   44  0.056778   42  0.372411   46  \n",
       "46   48  0.007431   58  0.371770   47  \n",
       "47   46  0.040546   43  0.366141   48  \n",
       "48   47  0.038234   44  0.365326   49  \n",
       "49   55  0.367250   22  0.362790   50  \n",
       "50   54  0.067962   41  0.338441   51  \n",
       "51   49  0.012702   55  0.332078   52  \n",
       "52   50  0.013703   54  0.328941   53  \n",
       "53   57  0.075562   39  0.325501   54  \n",
       "54   51  0.028615   45  0.309193   55  \n",
       "55   52  0.015924   53  0.291157   56  \n",
       "56   58  0.075191   40  0.288027   57  \n",
       "57   56  0.027800   46  0.279593   58  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assume df has columns: [\"model\", \"Method\", \"dataset\", \"prr\"]\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "tables = {}\n",
    "for model, df_model in df.groupby(\"model\"):\n",
    "    # pivot to wide form\n",
    "    df_wide = df_model.pivot_table(\n",
    "        index=\"Method\", columns=\"dataset\", values=\"prr\", aggfunc=\"mean\"\n",
    "    )\n",
    "\n",
    "    # add mean across datasets\n",
    "    df_wide[\"mean\"] = df_wide.mean(axis=1)\n",
    "\n",
    "    # build a new DataFrame with MultiIndex columns: (dataset, \"score\") and (dataset, \"rank\")\n",
    "    df_nice = {}\n",
    "    for col in df_wide.columns:\n",
    "        if col != \"mean\":\n",
    "            df_nice[(col, \"score\")] = df_wide[col]\n",
    "            df_nice[(col, \"rank\")] = df_wide[col].rank(method=\"min\", ascending=False).astype(int)\n",
    "\n",
    "    # add mean score + rank\n",
    "    df_nice[(\"mean\", \"score\")] = df_wide[\"mean\"]\n",
    "    df_nice[(\"mean\", \"rank\")] = df_wide[\"mean\"].rank(method=\"min\", ascending=False).astype(int)\n",
    "\n",
    "    # convert dict to DataFrame\n",
    "    df_pretty = pd.DataFrame(df_nice, index=df_wide.index)\n",
    "\n",
    "    # sort by mean rank\n",
    "    df_pretty = df_pretty.sort_values((\"mean\", \"rank\")).reset_index()\n",
    "\n",
    "    tables[model] = df_pretty\n",
    "    filename = f\"../resources/llm_resources/{model}_results.csv\"\n",
    "    df_pretty.to_csv(filename)\n",
    "\n",
    "# Example: show table for one model\n",
    "tables[\"llama8b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== llama8b :: coqa_no_context — Top 5 ===\n",
      "                 Method      prr  rank\n",
      " GreedyAveDissimilarity 0.391002     1\n",
      " IMBA_2_Exp_FeatureWise 0.382852     2\n",
      "IMBA_2_Beta_FeatureWise 0.380607     3\n",
      "IMBA_4_Beta_FeatureWise 0.379941     4\n",
      " IMBA_4_Exp_FeatureWise 0.379458     5\n",
      "\n",
      "=== llama8b :: gsm8k_cot — Top 5 ===\n",
      "                                   Method      prr  rank\n",
      "                 Master_1_Exp_FeatureWise 0.438419     1\n",
      "                   IMBA_2_Exp_FeatureWise 0.435330     2\n",
      "GreedySemanticEnrichedMTEAveDissimilarity 0.433381     3\n",
      "                Master_1_Beta_FeatureWise 0.430522     4\n",
      "                   IMBA_4_Exp_FeatureWise 0.430302     5\n",
      "\n",
      "=== llama8b :: mmlu — Top 5 ===\n",
      "                                       Method      prr  rank\n",
      "                   MaximumSequenceProbability 0.516374     1\n",
      "                            IMBA_5_Exp_Global 0.492882     2\n",
      "GreedySemanticEnrichedMaxprobAveDissimilarity 0.491508     3\n",
      "                           IMBA_5_Beta_Global 0.491450     4\n",
      "                            IMBA_4_Exp_Global 0.489219     5\n",
      "\n",
      "=== llama8b :: trivia — Top 5 ===\n",
      "                 Method      prr  rank\n",
      "     IMBA_3_Beta_Global 0.626566     1\n",
      " IMBA_2_Exp_FeatureWise 0.625140     2\n",
      "      IMBA_3_Exp_Global 0.624717     3\n",
      "IMBA_3_Beta_FeatureWise 0.623715     4\n",
      " IMBA_3_Exp_FeatureWise 0.623697     5\n",
      "\n",
      "=== llama8b :: wmt14_fren — Top 5 ===\n",
      "                   Method      prr  rank\n",
      "  IMBA_5_Beta_FeatureWise 0.475688     1\n",
      "   IMBA_5_Exp_FeatureWise 0.475661     2\n",
      "Master_1_Beta_FeatureWise 0.470590     3\n",
      " Master_1_Exp_FeatureWise 0.470549     4\n",
      "      Master_1_Exp_Global 0.459690     5\n",
      "\n",
      "=== llama8b :: wmt19_deen — Top 5 ===\n",
      "                       Method      prr  rank\n",
      "      IMBA_5_Beta_FeatureWise 0.613109     1\n",
      "       IMBA_5_Exp_FeatureWise 0.612408     2\n",
      "CoCoA_like_1_Beta_FeatureWise 0.611143     3\n",
      " CoCoA_like_1_Exp_FeatureWise 0.602976     4\n",
      "          Master_1_Exp_Global 0.595384     5\n",
      "\n",
      "=== llama8b :: xsum — Top 5 ===\n",
      "                   Method      prr  rank\n",
      "  IMBA_4_Beta_FeatureWise 0.414785     1\n",
      "Master_1_Beta_FeatureWise 0.410731     2\n",
      "   IMBA_4_Exp_FeatureWise 0.410475     3\n",
      "  IMBA_2_Beta_FeatureWise 0.410099     4\n",
      "  IMBA_5_Beta_FeatureWise 0.409612     5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assume df has columns: [\"model\", \"Method\", \"dataset\", \"prr\"]\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "# 1) Aggregate to one score per (model, dataset, method)\n",
    "agg = (\n",
    "    df.groupby([\"model\", \"dataset\", \"Method\"], as_index=False)[\"prr\"]\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "# 2) Rank methods within each (model, dataset), highest prr = rank 1\n",
    "agg[\"rank\"] = (\n",
    "    agg.groupby([\"model\", \"dataset\"])[\"prr\"]\n",
    "       .rank(method=\"min\", ascending=False)\n",
    "       .astype(int)\n",
    ")\n",
    "\n",
    "# 3) Keep only top K\n",
    "topk = agg[agg[\"rank\"] <= TOP_K].copy()\n",
    "\n",
    "# 4) Build a nested dict: { model: { dataset: DataFrame(...) } }\n",
    "topk_tables = {}\n",
    "for model, df_m in topk.groupby(\"model\"):\n",
    "    model_dict = {}\n",
    "    for ds, df_ds in df_m.groupby(\"dataset\"):\n",
    "        model_dict[ds] = (\n",
    "            df_ds[[\"Method\", \"prr\", \"rank\"]]\n",
    "            .sort_values([\"rank\", \"prr\"], ascending=[True, False])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    topk_tables[model] = model_dict\n",
    "\n",
    "# 5) Example: print top-5 for one model across all datasets\n",
    "model = \"llama8b\"\n",
    "for ds in sorted(topk_tables[model].keys()):\n",
    "    print(f\"\\n=== {model} :: {ds} — Top {TOP_K} ===\")\n",
    "    print(topk_tables[model][ds].to_string(index=False))\n",
    "\n",
    "# 6) (Optional) Save each dataset’s top-5 per model to CSVs\n",
    "for model, ds_dict in topk_tables.items():\n",
    "    for ds, df_ds in ds_dict.items():\n",
    "        out = f\"../resources/llm_resources/{model}_{ds}_top{TOP_K}.csv\"\n",
    "        df_ds.to_csv(out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multidimensional-uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
