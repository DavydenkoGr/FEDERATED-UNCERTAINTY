{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692de729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lm_polygraph.utils import UEManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d370c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f88df5d69c0>]\n"
     ]
    }
   ],
   "source": [
    "man_cal = UEManager.load('llama8b_xsum.man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9788454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sequence', 'GreedySemanticEnrichedPPLAveDissimilarity')\n",
      "('sequence', 'GreedySemanticEnrichedMaxprobAveDissimilarity')\n",
      "('sequence', 'GreedySemanticEnrichedMTEAveDissimilarity')\n",
      "('sequence', 'BestSemanticEnrichedPPLAveDissimilarity')\n",
      "('sequence', 'BestSemanticEnrichedMaxprobAveDissimilarity')\n",
      "('sequence', 'BestSemanticEnrichedMTEAveDissimilarity')\n",
      "('sequence', 'MbrSemanticEnrichedPPLAveDissimilarity')\n",
      "('sequence', 'MbrSemanticEnrichedMaxprobAveDissimilarity')\n",
      "('sequence', 'MbrSemanticEnrichedMTEAveDissimilarity')\n",
      "('sequence', 'SemanticEntropy')\n",
      "('sequence', 'SAR_t0.001')\n",
      "('sequence', 'MaximumSequenceProbability')\n",
      "('sequence', 'Perplexity')\n",
      "('sequence', 'MeanTokenEntropy')\n",
      "('sequence', 'BestSampledMaximumSequenceProbability')\n",
      "('sequence', 'BestSampledPerplexity')\n",
      "('sequence', 'BestSampledMeanTokenEntropy')\n",
      "('sequence', 'MbrSampledMaximumSequenceProbability')\n",
      "('sequence', 'MbrSampledPerplexity')\n",
      "('sequence', 'MbrSampledMeanTokenEntropy')\n",
      "('sequence', 'MonteCarloSequenceEntropy')\n",
      "('sequence', 'MonteCarloNormalizedSequenceEntropy')\n",
      "('sequence', 'CEDegMat')\n",
      "('sequence', 'GreedyAveDissimilarity')\n",
      "('sequence', 'BestAveDissimilarity')\n",
      "('sequence', 'MbrAveDissimilarity')\n",
      "('sequence', 'EigValLaplacian_NLI_score_entail')\n",
      "('sequence', 'DegMat_NLI_score_entail')\n"
     ]
    }
   ],
   "source": [
    "for el in man_cal.estimations.keys():\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564a6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdu.unc.entropic_ot import EntropicOTOrdering\n",
    "from mdu.unc.constants import OTTarget, SamplingMethod, ScalingType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46170e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    'llama8b_xsum.man',\n",
    "    'llama8b_wmt14_fren.man',\n",
    "    'llama8b_wmt19_deen.man',\n",
    "    'llama8b_trivia.man',\n",
    "    'llama8b_mmlu.man',\n",
    "    'llama8b_gsm8k_cot.man',\n",
    "    'llama8b_coqa_no_context.man',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38bf971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_list = [\n",
    "    {\n",
    "        'target': OTTarget.EXP,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.FEATURE_WISE,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "    {\n",
    "        'target': OTTarget.EXP,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.GLOBAL,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "    {\n",
    "        'target': OTTarget.BETA,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.FEATURE_WISE,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "    {\n",
    "        'target': OTTarget.BETA,\n",
    "        'sampling_method': SamplingMethod.GRID,\n",
    "        'scaling_type': ScalingType.GLOBAL,\n",
    "        'grid_size': 5,\n",
    "        'n_targets_multiplier': 1,\n",
    "        'eps': 0.5,\n",
    "        'max_iters': 1000,\n",
    "        'tol': 1e-6,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e371a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # [f\"only_confidence_{target.value}_{scaling_type.value}\", (('sequence', 'MaximumSequenceProbability'), ('sequence', 'Perplexity'), ('sequence', 'MeanTokenEntropy'), ('sequence', 'MonteCarloSequenceEntropy'), ('sequence', 'MonteCarloNormalizedSequenceEntropy'))],\n",
    "            # [f\"only_consistency_1_{target.value}_{scaling_type.value}\", (('sequence', 'EigValLaplacian_NLI_score_entail'), ('sequence', 'DegMat_NLI_score_entail'))],\n",
    "            # [f\"only_consistency_2_{target.value}_{scaling_type.value}\", (('sequence', 'GreedySemanticEnrichedPPLAveDissimilarity'), ('sequence', 'GreedySemanticEnrichedMaxprobAveDissimilarity'), ('sequence', 'GreedySemanticEnrichedMTEAveDissimilarity'), ('sequence', 'BestSemanticEnrichedPPLAveDissimilarity'), ('sequence', 'BestSemanticEnrichedMaxprobAveDissimilarity'), ('sequence', 'BestSemanticEnrichedMTEAveDissimilarity'), ('sequence', 'MbrSemanticEnrichedPPLAveDissimilarity'), ('sequence', 'MbrSemanticEnrichedMaxprobAveDissimilarity'), ('sequence', 'MbrSemanticEnrichedMTEAveDissimilarity'))],\n",
    "            # [f\"comb_1_{target.value}_{scaling_type.value}\", (('sequence', 'GreedyAveDissimilarity'), ('sequence', 'MaximumSequenceProbability'))],\n",
    "            # [f\"comb_2_{target.value}_{scaling_type.value}\", (('sequence', 'GreedyAveDissimilarity'), ('sequence', 'Perplexity'))],\n",
    "            # [f\"comb_3_{target.value}_{scaling_type.value}\", (('sequence', 'GreedyAveDissimilarity'), ('sequence', 'MeanTokenEntropy'))],\n",
    "            # [f\"comb_4_{target.value}_{scaling_type.value}\", (('sequence', 'GreedyAveDissimilarity'), ('sequence', 'Perplexity'), ('sequence', 'MeanTokenEntropy'), ('sequence', 'MaximumSequenceProbability'))],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc27c8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f88de845ee0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/ot/bregman/_sinkhorn.py:903: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\n",
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f88de0dad20>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/ot/bregman/_sinkhorn.py:903: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\n",
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f88dda386b0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/ot/bregman/_sinkhorn.py:903: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\n",
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f887cd4b380>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/ot/bregman/_sinkhorn.py:903: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\n",
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f88dcc07d40>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f88de963140>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/lm_polygraph/utils/manager.py:705: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_dict = torch.load(load_path)\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f88dda3a300>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/github/multidimensional_uncertainty/.venv/lib/python3.12/site-packages/ot/bregman/_sinkhorn.py:903: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "calib_ratio = 0.25\n",
    "import pickle\n",
    "\n",
    "for file_name in file_names:\n",
    "    man_current = UEManager.load(file_name)\n",
    "\n",
    "    measures_dict = {k: v for k, v in man_current.estimations.items()}\n",
    "\n",
    "    measures_dict[('sequence', 'CoCoA_greedy_MSP')] = man_current.estimations[('sequence', 'GreedyAveDissimilarity')] * man_current.estimations[('sequence', 'MaximumSequenceProbability')]\n",
    "    measures_dict[('sequence', 'CoCoA_greedy_PPL')] = man_current.estimations[('sequence', 'GreedyAveDissimilarity')] * man_current.estimations[('sequence', 'Perplexity')]\n",
    "    measures_dict[('sequence', 'CoCoA_greedy_MTE')] = man_current.estimations[('sequence', 'GreedyAveDissimilarity')] * man_current.estimations[('sequence', 'MeanTokenEntropy')]\n",
    "\n",
    "    measures_dict[('sequence', 'CoCoA_best_MSP')] = man_current.estimations[('sequence', 'BestAveDissimilarity')] * man_current.estimations[('sequence', 'BestSampledMaximumSequenceProbability')]\n",
    "    measures_dict[('sequence', 'CoCoA_best_PPL')] = man_current.estimations[('sequence', 'BestAveDissimilarity')] * man_current.estimations[('sequence', 'BestSampledPerplexity')]\n",
    "    measures_dict[('sequence', 'CoCoA_best_MTE')] = man_current.estimations[('sequence', 'BestAveDissimilarity')] * man_current.estimations[('sequence', 'BestSampledMeanTokenEntropy')]\n",
    "\n",
    "    n_samples = len(man_current.estimations[('sequence', 'Perplexity')])\n",
    "    np.random.seed(42)\n",
    "    calib_mask = np.random.choice(n_samples, size=int(n_samples * calib_ratio), replace=False)\n",
    "\n",
    "    final_measures = {}\n",
    "\n",
    "    for hyperparams in hyperparams_list:\n",
    "        target = hyperparams['target']\n",
    "        sampling_method = hyperparams['sampling_method']\n",
    "        scaling_type = hyperparams['scaling_type']\n",
    "        grid_size = hyperparams['grid_size']\n",
    "        n_targets_multiplier = hyperparams['n_targets_multiplier']\n",
    "        eps = hyperparams['eps']\n",
    "        max_iters = hyperparams['max_iters']\n",
    "        tol = hyperparams['tol']\n",
    "\n",
    "\n",
    "        model = EntropicOTOrdering(\n",
    "            target=target,\n",
    "            sampling_method=sampling_method,\n",
    "            scaling_type=scaling_type,\n",
    "            grid_size=grid_size,\n",
    "            target_params={},\n",
    "            eps=eps,\n",
    "            n_targets_multiplier=n_targets_multiplier,\n",
    "            max_iters=max_iters,\n",
    "            random_state=42,\n",
    "            tol=tol,\n",
    "        )\n",
    "\n",
    "        combinations = [\n",
    "            (\n",
    "                f\"CoCoA_like_1_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'GreedyAveDissimilarity'),\n",
    "                    ('sequence', 'BestAveDissimilarity'),\n",
    "                    ('sequence', 'BestSampledMaximumSequenceProbability'),\n",
    "                    ('sequence', 'BestSampledPerplexity'),\n",
    "                    ('sequence', 'BestSampledMeanTokenEntropy'),\n",
    "                    ('sequence', 'MaximumSequenceProbability'),\n",
    "                    ('sequence', 'Perplexity'),\n",
    "                    ('sequence', 'MeanTokenEntropy'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"CoCoA_like_2_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'BestAveDissimilarity'),\n",
    "                    ('sequence', 'BestSampledMaximumSequenceProbability'),\n",
    "                    ('sequence', 'BestSampledPerplexity'),\n",
    "                    ('sequence', 'BestSampledMeanTokenEntropy'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"CoCoA_like_3_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'GreedyAveDissimilarity'),\n",
    "                    ('sequence', 'MaximumSequenceProbability'),\n",
    "                    ('sequence', 'Perplexity'),\n",
    "                    ('sequence', 'MeanTokenEntropy'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"Master_1_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                    ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                    ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"Master_1_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                    ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                    ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"Master_2_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'CoCoA_best_MSP'),\n",
    "                    ('sequence', 'CoCoA_best_PPL'),\n",
    "                    ('sequence', 'CoCoA_best_MTE'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"Master_3_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'CoCoA_best_MSP'),\n",
    "                    ('sequence', 'CoCoA_best_PPL'),\n",
    "                    ('sequence', 'CoCoA_best_MTE'),\n",
    "                    ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                    ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                    ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"IMBA_1_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'CoCoA_best_MSP'),\n",
    "                    ('sequence', 'CoCoA_best_PPL'),\n",
    "                    ('sequence', 'DegMat_NLI_score_entail'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"IMBA_2_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                    ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                    ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                    ('sequence', 'DegMat_NLI_score_entail'),\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                f\"IMBA_3_{target.value}_{scaling_type.value}\", \n",
    "                (\n",
    "                    ('sequence', 'CoCoA_best_MSP'),\n",
    "                    ('sequence', 'CoCoA_best_PPL'),\n",
    "                    ('sequence', 'CoCoA_best_MTE'),\n",
    "                    ('sequence', 'CoCoA_greedy_MSP'),\n",
    "                    ('sequence', 'CoCoA_greedy_PPL'),\n",
    "                    ('sequence', 'CoCoA_greedy_MTE'),\n",
    "                    ('sequence', 'DegMat_NLI_score_entail'),\n",
    "                    ('sequence', 'EigValLaplacian_NLI_score_entail'),\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        for comb_list in combinations:\n",
    "            name, comb = comb_list[0], comb_list[1]\n",
    "            components = [measures_dict[el] for el in comb]\n",
    "            \n",
    "            calib_components = [np.array(comp)[calib_mask] for comp in components]\n",
    "\n",
    "            stacked_calib_components = np.column_stack(calib_components)\n",
    "            stacked_test_components = np.column_stack(components)\n",
    "\n",
    "            model.fit(stacked_calib_components)\n",
    "            scores = model.predict(stacked_test_components)\n",
    "\n",
    "            final_measures[name] = scores\n",
    "\n",
    "    with open(f'final_measures_{file_name.split(\".\")[0]}.pkl', 'wb') as f:\n",
    "        pickle.dump(final_measures, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d22ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b492b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
